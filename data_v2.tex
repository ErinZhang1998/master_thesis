
\subsection{Overview}
In response to the pilot results, we reconsider the data collection pipeline. 
We examined existing sketch datasets to see how their annotations could facilitate our data collection (dataset details are explained in Section \ref{relatedWorkChapter}).    
To shorten the time spent on sketching, we no longer asked annotators to sketch and instead only asked them to describe sketches in the QuickDraw dataset. Although we were no longer able to design text prompts ourselves and collect creative sketches like the ones in the previous pilot, we solved the problem that it was difficult to understand how some sketches illustrate the give prompts. 

The most important objective that this dataset should fulfill is allowing us to study how sketches share similar semantic parts and descriptions that are adapted to be used for different objects. Therefore, we must resolve the issue that some text descriptions we collected in the pilot either did not describe everything drawn in a step or described more than what was drawn.  
% To solve this problem of misaligned text descriptions and drawings, 
Therefore, we used semantic part annotations from the Sketch Perceptual Grouping (SPG) dataset, which provides semantic part labels for each stroke in $20,000$ sketches from the QuickDraw dataset. In this way, annotators did not need to spend time thinking about how to segment sketches into parts themselves. 

Moreover, to help annotators coming up with creative ways to describe the semantic parts, in each task, we presented a pair of sketches with contrasting features, implicitly priming them to describe the visual differences. 
% Between  and SketchSeg, both containing annotaiton for semantically meaningful parts in sketches, 
% SPG annotates for QuickDraw sketches while SketchSeg collects its own sketches. We picked SPG, since it will be easier in the future to extend our datasets given the large QuickDraw reservoir of sketches. 
% Moreover, SketchSeg dataset contains a \textit{fourleg} category that includes many different kinds of animals, such as horse, sheep, and cow, but the QuickDraw categories are more fine-grained, so form a model learning perspective, SPG will also be more generalizable.  


\begin{figure*}[!htb]
\begin{subfigure}{\textwidth}
  \centering
  % include first image
  \includegraphics[width=.8\linewidth]{data_collection/pilot_02_01_annotation_1.png}  
  \caption{Design of main task for first pilot.}
  \label{v2.main_task.1.a}
\end{subfigure}
\newline
\begin{subfigure}{\textwidth}
  \centering
  % include third image
  \includegraphics[width=.8\linewidth]{data_collection/pilot_02_04_annotation.png}  
  \caption{Design of main task for second pilot.}
  \label{v2.main_task.1.b}
\end{subfigure}
\end{figure*}

\begin{figure*}[!htb]
\ContinuedFloat
\begin{subfigure}{\textwidth}
  \centering
  % include third image
  \includegraphics[width=.8\linewidth]{data_collection/pilot_02_04_annotation.png}  
  \caption{Design of main task for third pilot.}
  \label{v2.main_task.1.c}
\end{subfigure}
\newline
\begin{subfigure}{\textwidth}
  \centering
  % include third image
  \includegraphics[width=.8\linewidth]{data_collection/pilot_02_04_annotation.png}  
  \caption{Design of main task for final task.}
  \label{v2.main_task.1.d}
\end{subfigure}
\caption{Progress of the design two for the main task in version two.}
\label{v2.main_task.1}
\end{figure*}

\subsubsection{Main Task Interface}
When collecting the previous prompt-guided dataset, we relied on testing the interface with students in the lab to determine our design, but we observed performance differences between the students and turkers, such as amount of variety in the sketches, amount of time spent on the task, and common confusions related to understanding the requirements. 
Therefore, when collecting the contrasting sketch text dataset, we deployed several pilots on AMT to design the new interface.  
In Figure \ref{v2.main_task.1}, we show how the main task interface progressed from the first pilot to the final version used to collect the entire dataset. 

To better study how similar words are used differently across sketches, we restricted the annotations from whole sentences (Figure \ref{v2.main_task.1.a}) to only adjective phrases (Figure \ref{v2.main_task.1.b}, \ref{v2.main_task.1.c}, \ref{v2.main_task.1.d}). 
We juxtaposed two sketches and highlighted the parts to be annotated in different colors to help annotators notice the contrasting features between the two sketches. 
Moreover, this design expedited the annotation process, since it was easier for people to perform contrasting tasks than to generate descriptions from a single sketch. 

% At the beginning, we stated explicitly that annotators should describe the differences between the sketches (\textit{Describe differences} in \ref{v2.main_task.1.a} and \textit{Compared to Sketch 1/2} in \ref{v2.main_task.1.b}), but we received many annotations that contain comparative and superlatives, so we eventually only have a blank without any introductory phrases to overly emphasize that the goal of the tasks is to create a dataset of contrastive pairs of descriptions, and the juxtaposition is meant only as a mental hint to ease annotation.

\subsubsection{Instruction and Requirement}
At the beginning, the instruction limited the annotators to provide three types of descriptions: shape, size, and position. 
However, in order to collect creative descriptions, we lifted restrictions on the type of words and only required annotators to fill in the blank with adjective phrases. We also provided some examples of adjective phrases in common sentences, unrelated to our task, for annotators to better understand their usage [FIGURE]. 

% In this version, the advantage is that since we have greatly simplify the task to only providing the textual descriptions, the turkers do not have to spend time coming up with drawings for a \textit{adjective}$\times$\textit{noun} prompt, and they do not have to put effort into keeping track of their drawing process to decide how to divide the drawing process into steps and then annotate for each step. 
Since we simplified the HIT from 3 sub-tasks, sketching for the prompt, segmenting the sketches, and describing each step, to only asking for part descriptions, the requirements are much easier to write. 
We did not struggle with explaining what semantic parts in sketches are and how they should be described like previously.  
We relied on the examples in the instruction to give annotators an idea of what descriptions we want. Some examples that we used in the tasks are shown in [FIGURE].
However, the downside for doing so is that the vocabularies used in by the annotators are primed by those in the examples, and we see that annotators would tend to repeat these vocabularies. Therefore, we especially added the requirement that states the annotators are not limited to words used in the examples, and they should use any words that can illustrate the parts well. The full set of requirements used in the final version is shown in Figure \ref{v2.requirement.1}.

\begin{figure*}[!htb]
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{pantanal.jpeg}  
  \caption{Design of main task for first pilot.}
  \label{v2.requirement.examples.1}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{pantanal.jpeg}  
  \caption{Design of main task for first pilot.}
  \label{v2.requirement.examples.2}
\end{subfigure}
\newline
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{pantanal.jpeg}  
  \caption{Design of main task for second pilot.}
  \label{v2.requirement.examples.3}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{pantanal.jpeg}  
  \caption{Design of main task for second pilot.}
  \label{v2.requirement.examples.4}
\end{subfigure}
\newline
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{pantanal.jpeg}  
  \caption{Design of main task for second pilot.}
  \label{v2.requirement.examples.5}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{pantanal.jpeg}  
  \caption{Design of main task for second pilot.}
  \label{v2.requirement.examples.6}
\end{subfigure}
\caption{Progress of the design two for the main task in version two.}
\label{v2.requirement.examples}
\end{figure*}

\begin{figure*}[h]
\includegraphics[width=.8\linewidth]{pantanal.jpeg}  
\caption{The set of requirements used in the final task.}
\label{v2.requirement.1}
\end{figure*}

The requirement that was a bit challenging for people to understand was the one regarding
\textit{Do not use adjectives related to personal opinions, such as random, good, messy, beautiful, and strage, that are hard to achieve consensus if others were to validate your answers.}. Since we hope that the model can get signal from the texts about what kind of figures to draw, words that do not directly convey visual properties of the parts are not helpful. We later changed the wording to \textit{Do not use adjectives that fail to describe specific visual properties of the objects in the sketches}. A slight caveat here is that we actually hope to collect descriptions that describe the emotions expressed in the sketches. We know beforehand that we hope to collect a dataset for the \textit{face} category, so it is quite common for faces to express emotions like happy and sad, and we were slightly worried that some turkers might consider these words as not illustrating enough visual properties about the drawings, since they are quite abstract, at least compared to adjectives like \textit{rectangular} or \textit{wide}. 

\input{data_v2_qual.tex}

\input{data_v2_deploy.tex}