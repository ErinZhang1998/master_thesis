
\subsection{Deployment Results}
% In order to determine how feasible the task is, we first deployed a version among lab members, and we obtained 55 drawings along with their annotations. All 55 drawings are shown in Figure v1.results.2. Some examples of step annotations are shown in Figure v1.results.3.  
% [Figure v1.results.2: 55 drawings from lab deployment (see jupyter notebook oct\_28\_trial\_analysis)]
% [Figure v1.results.3: 3 examples of drawings with steps?]
In our first pilot, we used prompts in the forms of \textit{adjective}$\times$\textit{noun}. 
The list of adjectives includes: \textit{happy, sad, surprised, sleepy, love-struck, evil}; the list of nouns includes: 
\textit{person, kid, cat, bear, dog, sheep, jellyfish, cup of bubble tea, apple, burger, sun, moon, star}. 
We want to see what sketches and text descriptions annotators would provide for prompts that ask for imaginative beings not in this world and include novel compositions of unrelated concepts, such as \textit{evil apple} or \textit{love-struck moon}. 
With these creative prompts, we hope to collect data that contain interesting compositions of the same geometric shapes and descriptions across different objects. 
We can then learn models that can, for example, generate circles to be different parts in different objects: eyes, moon, cherries, and angel halo. 

Since we only collected 55 sketches, we were able to manually examine every sketch, and we found many creative sketches [FIGURE]. 
However, one issue was that turkers took a long time, on average 30 minutes, to complete one sketch and provide descriptions. 

The second problem was that it was difficult to understand how some annotators interpreted the prompts through their sketches. [FIGURE]
Indeed, sketching is by its nature very subjective, a common challenge in creative AI.    

The third problem, the most concerning one, was that the part descriptions did not align well with the sketches: some annotators failed to describe every part they drew in a step, or they described parts not in the annotated step. [FIGURE]    


% [Figure v1.results.4: drawings from the amt pilot]
% [Figure v1.results.1: a: oct 28 lab deployment. b: dec 28 amt deployment]
% [Table v1.results.1: comparing the statistics of lab vs. amt deployment]
% Histograms of time each annotator spent on the task is illustrated in Figure v1.results.1. Statistics of the distributions are shown in Table v1.results.1. 
% The discrepancy might be caused by the fact that lab members with their background in computer science have implicit understandings of what kind of quality data are needed to train ML models.   