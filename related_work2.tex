% In the space of sketch representation learning. After we have settled on human-robot collaborative sketching, we surveyed the field for existing sketch datasets and what they contain, what they lack, and what gap does our work fill. 
Past works have learned sketch representations from a wide variety of tasks: sketch recognition, sketch generation from image, image generation from sketches, sketch retrieval of 3D objects, sketch retrieval of images, semantic segmentation of sketches, etc. 
% Essentially, one can perform any tasks that are done on images and explore the techniques for sketches. 
The survey paper by \citet{sketchsurvey} provides a comprehensive overview and systematic taxonomy of . 

There is a wide range of tasks that can be done on sketches, both unimodal and multimodal, and, for each task, a large reservoir of deep learning methods used to solve the tasks. 
\citet{sketchsurvey} gives a comprehensive review of the task taxonomy, summarized the unique challenges associated with each individual tasks, and evaluated the different deep learning methods on sketch recognition through a library \texttt{TorchSketch} the authors wrote, and it contains implementation for CNN, RNN, GNN, and TCN. The sections that are most relelvant to us are: sketch generation, sketch segmentation. Sketch generation because we are trying to learn a generative model. Sketch segmentation because we are trying to gain insight about how are semantically meaningful units discovered in sketches and what relationships do the parts have with the whole sketch.      
Similar to images, sketches have hierarchical structure, and we 

% The hope is that we can leverage previous work on sketch representation learning to gain insights about sketches and how to learn good representations of them. What is unique about sketches compared to regular RGB images from, for example, ImageNet is that (1) sketches are abstract characterisation of the objects, and although humans can recognize and understand a sketch perfectly, they do not necessarily bear big resemblance to their image counterpart; therefore, methods that work well on RGB images, especially generative models like GAN that have successfully generated wide range of images from texts, a realm that we care about, it is not necessarily the case that they can generalize well to our dataset.  



%%%%%%%%%%%% outputting basic semantically meaningful components
In the larger realm of RGB images:
Therefore, our dataset will be a good benchmark for how well these models work at capturing the individual semantic components of an object. The reason that we claim this is that some work on GAN's have try to look at how to manipulate certain regions in the images by manipulating the latent space. While this line of work also try to look at how .This area of the work is around facial feature editting. Work such as Semantic Photo Manipulation with a Generative Image Prior \citep{Bau:Ganpaint:2019}, has an interactive interface where the user can use stroke to indicate where in the image they would want a certain object, and the GAN will generate the objects in that location. ``semantic image editing tasks, including synthesizing new objects consistent with background, removing unwanted objects, and changing the appearance of an object''. semantic edit on an object. They would apply a semantic vector space operation in the latent space. How our work is different from this work is that: how well the methods can work on sketches and how well can the edits can done through language. [?] Moreover, it seems like we need to have an image already in order to do the manipulation, but for our ideal tasks, we start from a blank canvas.

