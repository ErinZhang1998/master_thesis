
 
Robotics have advanced significantly in the past 30 years, and while at the beginning, robots work in factories on assembly line tasks and are far from our daily lives, they have been moving closer to us and into our homes: from iRobot roomba that can sweep floors to Labrador assistive robots that can navigate around and retrieve heavy loads. 
In the future, we would want to communicate with robots and collaborate with them on tasks, similar to how we smoothly interact with other people in completing daily chores.   
Out of the many tasks that we can collaborate with robots on, this thesis focuses on drawing. 
Creative AI, such as using deep learning models to generate paintings and music, has been a popular research domain, since creative activities are representative of unique human intelligence. 
Numerous works have attempted to replicate the creative process on machines.
For example, Pharmako-AI \citep{allado-mcdowell_okojie_2020} is a book co-written by K Allado-McDowell and GPT-3 \citep{gpt3} through exchanges between the human and the language model. 
Works like DALL-E, GLIDE, and DALL-E 2 tackle the problem of synthesizing images from text prompts, or short language descriptions, and these generative models have produced many imaginative and inspring art pieces. 
Since creative activities are carried out only in solitude, we attempt to investigate at the intersection of human-robot interaction (HRI) and creative AI. 
Similar to how people communicate ideas to each other, our research is motivated by the goal of producing creative machines that can interact and collaborate with people. 
% As humans, we sparked each other's imagination. Song writers compose songs together, painters seek inspiration from their peers. We communicate our ideas to others, and it would be nice if creative machines can communicate with us. The idea that machines have by themselves a creative voice and a way of executing, interpreting, expanding on the short language prompt is enigmatic.  
% There have not been a lack of work on drawing and creative art. Proliferous amount of work on generative models, not mentioning the countless GAN-based generative models, such as StyleGAN. With the emergence of CLIP, work like StyleCLIP utilizes the rich joint embeddings space of vision and language to create art works. 


More so, we are motivated by how kids draw. Children start drawing from an early age, even before their language system has fully established, they are using symbols to represent things they experience in this world. It is almost an instinctual activity that is carved into our nature. A clear progression from simple scribbles to sophisticated composition of shapes. What is more inspiring is how children are able to describe to adults their creations and in these exchanges, metaphorical expressions emerges, indicating that they understand the interaction of abstract shapes and concrete objects. This activity of projecting the high dimensional real-world experience down to the two-dimensional canvas showcases the achievement of human creativity. The completed processes that hide beneath the simple stroke lines of children art are yet to be understood. Inspired by this intriguing activity, we want to build robots that can draw with us, take as input our descriptions of the objects being drawn, and understand sentences like I want to show a large head for the angel. We want humans and machines to all be part of the creative process.   

Even just in the realm of drawing with robot, there are a wide range of research directions: on one end, by focusing more on challenges with robot-arm manipulation, one can investigate creating highly realistic paintings through precise execution of a variety of paint tools   

% To study the topic of human-robot collaborative drawing, we start the project by pinpointing how should we define a specific research problem. If we lay out the task, drawing, on a spectrum, we have on one end tasks of constructing still-life sketches that are highly realistic and look as if they are transformed from real images. From a small experiment, where  




% Which aspect of drawing? Painting like Bob Ross? 

% Text representation. 
% We have RNN to model the sequence of strokes. Can we still use RNN to model the sequence of words? Does the length of the sentence matter? If we only have adjectives, how can we effectively model these words? 
% What are methods that can model the semantics of these words alone? 
% Where can we find existing representations of these words? How are traditional text-image synthesis methods modeling the words? 

% Reed et al. [28] obtain the text encoding of a textual description by using a pre-trained character-level convolutional recurrent neural network (char-CNN-RNN). The char-CNN-RNN is pretrained to learn a correspondence function between text and image based on the class labels. 

% by collecting a dataset of sketches that we can learn such model from. 

% First meeting with Oliver and Yonatan. 
% Between the two choices of task with long horizon, a long sequence of simple tasks, or short sequence of complicated tasks. 
% World. 

% \section{Exploring the Problem Space, early October} 

% Master Thesis Brainstorm

% The discussion starts in Oliver's office. I am trying to pinpoint where I want to bring in the ideas of natural language processing. What are some ideas that I wanted to explore.  


% The first question that we want to solve is what data should we collect in order to build a robot that can draw with humans? Inspired by drawing sessions created by Bob Ross, we have considered painting with robots and experimented briefly with oil painting on canvas with a Franka robot, but precise execution of brush strokes and the technical details surrounding manipulating brushes to create the desire scene are even very difficult for humans, and the focus of our research is to make a step towards human-robot collaboration on tasks, we do not want to be side-tracked by difficulties around robot control, which in itself is an extremely interesting problem that we wish to explore another time. 

% What inspired us to choose sketch was a session we conducted with another student studying Graphics. Initially, we set up the session to get a sense of how difficult it was to teach someone to draw with only language commands, and along the way, we discovered that generating real-looking sketches are very difficult without visual demonstrations from the instructors, but creating emoji-looking icons is more do-able. What is more interesting is that, icons are intrinsically abstract in that general geometric shapes like circles and rectangles are used for different objects and convey different meanings in different context. For example, a person can draw a circle to represent a left eye or to represent a face. Moreover, a face can be of different shapes: circular, oval, triangular, etc. This abstract nature of icon-like sketches is something interesting that we wish to explore, so we want to collect a dataset of sketches done by humans but, most importantly, we want to track the steps people take to complete them so that we can achieve the goal of drawing together with a robot.    