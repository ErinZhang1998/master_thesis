
 
Robotics have advanced significantly in the past 30 years, and while at the beginning, robots work in factories on assembly line tasks and are far from our daily lives, they have been moving closer to us and into our homes: from iRobot roomba that can sweep floors to Labrador assistive robots that can navigate around and retrieve heavy loads. 
In the future, we would want to communicate with robots and collaborate with them on tasks, similar to how we smoothly interact with other people in completing daily chores.   
Out of the many tasks that we can collaborate with robots on, this thesis focuses on drawing. 
Creative AI, such as using deep learning models to generate paintings and music, has been a popular research domain. Since creative activities are representative of unique human intelligence, numerous works have attempted to replicate the creative process on machines.
For example, Pharmako-AI \citep{allado-mcdowell_okojie_2020} is a book co-written by K Allado-McDowell and GPT-3 \citep{gpt3} through exchanges between the human and the language model. 
Works like DALL-E, GLIDE, and DALL-E 2 tackle the problem of synthesizing images from text prompts, or short language descriptions, and these generative models have produced many imaginative and inspring art pieces. 
Since creative activities are carried out not only in solitude, we attempt to investigate at the intersection of human-robot interaction (HRI) and creative AI. 
Similar to how people communicate ideas to each other, our research is motivated by the goal of producing creative machines that can interact and collaborate with people. 
% As humans, we sparked each other's imagination. Song writers compose songs together, painters seek inspiration from their peers. We communicate our ideas to others, and it would be nice if creative machines can communicate with us. The idea that machines have by themselves a creative voice and a way of executing, interpreting, expanding on the short language prompt is enigmatic.  
% There have not been a lack of work on drawing and creative art. Proliferous amount of work on generative models, not mentioning the countless GAN-based generative models, such as StyleGAN. With the emergence of CLIP, work like StyleCLIP utilizes the rich joint embeddings space of vision and language to create art works. 

There is a wide range of research directions to pursue in the domain of drawing with robot; on one end, by focusing more on challenges with robot-arm manipulation, one can investigate constructing highly realistic paintings through precise execution of a variety of painting tools. 
In this work, we focus on generating simple icon-like sketches, similar to the doodles collected through the Quick,Draw! game, which asks participants to draw an object under 20 seconds\footnote{To play the game: \url{https://quickdraw.withgoogle.com/}} \citep{ha2017neural}. 
To realize collaboration on drawing, we want to focus on language understanding for robotics and enabling people to participate in the creative process of aforementioned generative models. Doodling avoids learning sophisticated art techniques and allow people without art background to command the robot to draw simple graphics. Our project is interested in learning how people use language creatively to describe the sketches they want to create with a robot. 
Therefore, one of our main challenges is building a drawing system that can respond to unconstrained language commands, so that, in the future, users of home robots do not have to limit themselves to a set of instructions and can have more natural interactions with the robots.     

% More so, we are motivated by how kids draw. Children start drawing from an early age, even before their language system has fully established, they are using symbols to represent things they experience in this world. It is almost an instinctual activity that is carved into our nature. A clear progression from simple scribbles to sophisticated composition of shapes. What is more inspiring is how children are able to describe to adults their creations and in these exchanges, metaphorical expressions emerges, indicating that they understand the interaction of abstract shapes and concrete objects. This activity of projecting the high dimensional real-world experience down to the two-dimensional canvas showcases the achievement of human creativity. The completed processes that hide beneath the simple stroke lines of children art are yet to be understood. Inspired by this intriguing activity, we want to build robots that can draw with us, take as input our descriptions of the objects being drawn, and understand sentences like I want to show a large head for the angel. We want humans and machines to all be part of the creative process.   


% What inspired us to choose sketch was a session we conducted with another student studying Graphics. Initially, we set up the session to get a sense of how difficult it was to teach someone to draw with only language commands, and along the way, we discovered that generating real-looking sketches are very difficult without visual demonstrations from the instructors, but creating emoji-looking icons is more do-able. What is more interesting is that, icons are intrinsically abstract in that general geometric shapes like circles and rectangles are used for different objects and convey different meanings in different context. For example, a person can draw a circle to represent a left eye or to represent a face. Moreover, a face can be of different shapes: circular, oval, triangular, etc. This abstract nature of icon-like sketches is something interesting that we wish to explore, so we want to collect a dataset of sketches done by humans but, most importantly, we want to track the steps people take to complete them so that we can achieve the goal of drawing together with a robot.    