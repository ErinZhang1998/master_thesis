

Robotics have advanced significantly these years, the trend seems to be that while at the beginning, robots are in the far factory working on assembly line tasks, and they are very far from our lives; from the little iRobot roomba to more sophisticated labrador, which that can navigate your home to retrieve objects for you, or []. As the robot moves closer to us and into our lives, we would want the robots to feel not just like a machine that can help us, a device that we command around, but we would like the communication to go both ways so that robots can talk to us and complete tasks with us. In the many tasks that can be done in collaboration with robots, we investigate the task of drawing. The reason being that the creative realm is such a bold challenge, and it seems like creative activities like drawing have always being the defining mark of human intelligence, a sacred part of our experience that is hard to tap into and understand the root of this activity. On this end, numerous works have attempted to replicate the creative process on machines, allowing them to create paintings, write poetry, compose music, etc. One notable creation is GPT-3, and the book [] is co-authored by human writer and GPT-3. GPT-3 can also generate drawings from short poems. DALL-E, GLIDE, DALL-E 2 are all generative models that can generate increadibly creative work that even would make a human go, that's imaginative. Of course, creativity is not a solo activity, and in human society, creative activities are carried out in groups. As humans, we sparked each other's imagination. Song writers compose songs together, painters seek inspiration from their peers. We communicate our ideas to others, and it would be nice if creative machines can communicate with us. The idea that machines have by themselves a creative voice and a way of executing, interpreting, expanding on the short language prompt is enigmatic.  

More so, we are motivated by how kids draw. Children start drawing from an early age, even before their language system has fully established, they are using symbols to represent things they experience in this world. It is almost an instinctual activity that is carved into our nature. A clear progression from simple scribbles to sophisticated composition of shapes. What is more inspiring is how children are able to describe to adults their creations and in these exchanges, metaphorical expressions emerges, indicating that they understand the interaction of abstract shapes and concrete objects. This activity of projecting the high dimensional real-world experience down to the two-dimensional canvas showcases the achievement of human creativity. The completed processes that hide beneath the simple stroke lines of children art are yet to be understood. Inspired by this intriguing activity, we want to build robots that can draw with us, take as input our descriptions of the objects being drawn, and understand sentences like I want to show a large head for the angel. We want humans and machines to all be part of the creative process.   

There have not been a lack of work on drawing and creative art. Proliferous amount of work on generative models, not mentioning the countless GAN-based generative models, such as StyleGAN. With the emergence of CLIP, work like StyleCLIP utilizes the rich joint embeddings space of vision and language to create art works. To pinpoint 

To study the topic of human-robot collaborative drawing, we start the project by pinpointing how should we define a specific research problem. If we lay out the task, drawing, on a spectrum, we have on one end tasks of constructing still-life sketches that are highly realistic and look as if they are transformed from real images. From a small experiment, where  



To shed light on how I got interested in the problem of human-robot collaborative drawing. The topic of text-image synthesis has existed for a very long time. And the academia has also been quite interested in drawing 

Which aspect of drawing? Painting like Bob Ross? 

Text representation. 
We have RNN to model the sequence of strokes. Can we still use RNN to model the sequence of words? Does the length of the sentence matter? If we only have adjectives, how can we effectively model these words? What are methods that can model the semantics of these words alone? Where can we find existing representations of these words? How are traditional text-image synthesis methods modeling the words? 

Reed et al. [28] obtain the text encoding of a textual description by using a pre-trained character-level convolutional recurrent neural network (char-CNN-RNN). The char-CNN-RNN is pretrained to learn a correspondence function between text and image based on the class labels. 


by collecting a dataset of sketches that we can learn such model from. 

First meeting with Oliver and Yonatan. 
Between the two choices of task with long horizon, a long sequence of simple tasks, or short sequence of complicated tasks. 
World. 

% \section{Exploring the Problem Space, early October} 

Master Thesis Brainstorm

The discussion starts in Oliver's office. I am trying to pinpoint where I want to bring in the ideas of natural language processing. What are some ideas that I wanted to explore.  


The first question that we want to solve is what data should we collect in order to build a robot that can draw with humans? Inspired by drawing sessions created by Bob Ross, we have considered painting with robots and experimented briefly with oil painting on canvas with a Franka robot, but precise execution of brush strokes and the technical details surrounding manipulating brushes to create the desire scene are even very difficult for humans, and the focus of our research is to make a step towards human-robot collaboration on tasks, we do not want to be side-tracked by difficulties around robot control, which in itself is an extremely interesting problem that we wish to explore another time. 

What inspired us to choose sketch was a session we conducted with another student studying Graphics. Initially, we set up the session to get a sense of how difficult it was to teach someone to draw with only language commands, and along the way, we discovered that generating real-looking sketches are very difficult without visual demonstrations from the instructors, but creating emoji-looking icons is more do-able. What is more interesting is that, icons are intrinsically abstract in that general geometric shapes like circles and rectangles are used for different objects and convey different meanings in different context. For example, a person can draw a circle to represent a left eye or to represent a face. Moreover, a face can be of different shapes: circular, oval, triangular, etc. This abstract nature of icon-like sketches is something interesting that we wish to explore, so we want to collect a dataset of sketches done by humans but, most importantly, we want to track the steps people take to complete them so that we can achieve the goal of drawing together with a robot.    


\subsubsection{CLIP}
[]
