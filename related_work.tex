
In this chapter, we survey two areas of related work: sketch datasets and sketch generation. 
There has been a proliferation of work on sketch representation learning \citep{ha2017neural,  eitz2012hdhso,spg_paper, qmulDataset, doodlerGAN, kim2019codraw, aksan2020cose, ribeiro2020sketchformer, Sketchpix2seqPaper, sketchBertPaper}. 
To study how human compose descriptors and shapes in sketches, we look into what annotations current sketch datasets contain and found that they lack either semantic part labels or language descriptions of the parts (Section \ref{related.sketch.datasets}).    
These datasets were created to solve classical computer vision tasks, such as recognition, segmentation, retrieval, whole-sketch generation, etc. 
We will then look into generative models of sketches in Section \ref{related.sketch.generate}. Although there have been many works on generating natural images, sketch generation has its own unique challenges: sketches contain sparse information and might bear little resemblance to their natural image counterparts; moreover, they illustrate features of the objects in an abstract way, such as a triangle as the house roof or a letter \textit{V} as the ice-cream cone. 
% DoodlerGAN is a recent work that looks into generating sketches in a part-based manner \citep{doodlerGAN}, closely related to our interests in studying how parts are combined in sketches. It follows a long line of works that employ generative adversarial networks (GAN) for image generation.  
% It also belongs to a set of works set of works on generating images from text descriptions or use texts to manipulate image styles, but they often work with a constraint set of language, with the exception of industrial-scale generative models with billions of parameters trained from text-image dataset on the hundred million scale      
% There is a long line of works on generating realistic images by learning their underlying distributions, especially on employing generative adversarial networks (GAN). 
% Related to our work is a .
%  
% Therefore, we also look into the field of sketch representation learning: what methods have been employed and what datasets are constructed.   

% \section{Image Synthesis from Text Prompts}


% \section{Sketch Representation Learning}

\section{Sketch Dataset} \label{related.sketch.datasets}
% Since our work seeks to collect a dataset of (sketch parts, text description), we survey existing sketch datasets and find that they either lack language descriptions, for the entire sketch or for parts in a sketch, or they do not contain semantic object annotations. 

\paragraph{TU-Berlin}
\citet{eitz2012hdhso} is one of the first works to investigate the characteristics of free-hand sketches and attempt to extract local features based on orientation, which are later used in the task of sketch recognition. 
It also provides the TU-Berlin sketch dataset that contains $20,000$ sketches spanning 250 object categories with 80 samples in each. 
The TU-Berlin dataset is then extended for sketch-based 3D object retrieval to contain 1814 new sketches for 130 common household object categories. This additional set also contains hierarchical category labels; for example, in the label hierarchy, the descendant categories of the \textit{animal} category are: \textit{arthropod, biped, human, flying creature, quadruped,} and \textit{underwater creature}. 
The TU-Berlin dataset contains some of the highest quality sketches for a wide range of categories, but they lack both text descriptions and semantic part annotations, and our dataset contains both types of annotations, although only for simpler sketches in the face and angel category.    

\paragraph{Datasets with Text Descriptions}
The QMUL dataset contains sketches of shoes and chairs: 419 shoe sketches of various types of shoes, such as high heels, boots, and ballerina flats, and 297 chair sketches of different kinds \citep{qmulDataset}. 
Although QMUL annotates for attributes of the sketches, they come from a fixed set of descriptions and are obtained from product tags, so these descriptions do not reflect how humans would creatively describe the drawings on their mind, which is a feature of our collected dataset.    
The Sketchy dataset contains $75,000$ sketches of 125 categories, and each sketch is paired with an image; Sketchy is additionally annotated with sketch validity metrics and short descriptions from annotators \citep{sketchyDataset}. However, these descriptions are more like comments given by the participants, and the collecting process is not carefully designed, unlike our data collection process, where obtaining the text description is the main focus. 
% ensuring the quality of the language
Moreover, the short texts in Sketchy do not describe individual semantic object in the sketches.     

\paragraph{QuickDraw Dataset}
The QuickDraw dataset is one of the largest sketch datasets, containing 50 million sketches distributed across 345 object categories. Each category has about 100,000 sketches \citep{ha2017neural}. 
Annotators for the TU-Berlin dataset have 30 minutes to draw one sketch, and annotators for the QMUL dataset have reference photos for the sketches they would draw later; on the other hand, participants of Quick,Draw! have 20 seconds to create the sketches for a randomly assigned category without time to look for reference, so sketches in the QuickDraw dataset are the simplest. However, since sketches that failed to be recognized by a classification model are filtered, they are in general of good quality and are representative of the general population drawing skills. Moreover, many works on sketch representation learning use the QuickDraw dataset, so utilizing these sketches would allow us to adapt other pre-trained generative models to model our dataset. 
These features of the QuickDraw dataset make it very favorable to be used to learn a collaborative drawing robot that can interact with a wide range of users, including children. However, the QuickDraw dataset contains neither semantic part annotation nor language descriptions, and our collected dataset contains both.   

\paragraph{Datasets with Semantic Part Annotations}
The sketches in our dataset come from the \\
QuickDraw dataset, but the original QuickDraw dataset does not provide labels for semantic objects in the sketches. 
The Sketch Perceptual Grouping (SPG) dataset \citep{spg_paper} and the SketchSeg dataset \citep{sketchsegDataset} contain annotations for semantic segmentation, meaning that each stroke in a sketch is paired with a semantic label. For example, strokes in a sketch for the \textit{face} category are assigned one of the following labels: \textit{eyes, nose, mouth, ear, hair, moustache, outline of face}. 
The SPG dataset annotates for $25,000$ QuickDraw sketches, belonging to 25 (out of 345) categories, and it selects 800 out of the original $100,000$ sketches in each category for annotation. 
The SketchSeg dataset builds upon the SPG dataset by using a recurrent neural work to generate additional sketches stemmed from one sketch in SPG; since each stroke in the generated sketch corresponds to a stroke in the original sketch, it automatically has part annotations. However, since the quality of the generated sketch is dependent upon the generative model, we choose the original SPG dataset, which is also publicly available. 
The SPG dataset does not contain text descriptions like our dataset, so we cannot study how people describe their sketches using SPG alone. 
% so although they can be used to build a collaborative drawing system, such system would not have the capacity to respond to natural language commands from users, and our work is interested in learning collaborative robots that can communicate freely with people.    
%DoodlerGAN

\paragraph{SketchCUB}
\cite{sketchbirds} collects a dataset, SketchCUB, of sketches and captions by transforming realistic images in the CUB dataset \citep{WahCUB_200_2011} into sketches with holistically-nested network (HED) \citep{hedPaper}. These sketches in the SketchCUB dataset are not done by humans and might not be representative of how the general population sketch, compared to those in QuickDraw.
The SketchCUB dataset contains 10K sketches over 200 different categories of birds. Moreover, the original CUB dataset collects the captions by presenting to the annotators a fixed set of attributes and asking them to determine whether the bird has the attribute or not. There are a total of 312 binary attributes, and they are traditionally used to determine species of birds in nature. Therefore, the captions might not represent how people describe sketch parts. 
The SketchCUB text descriptions are also for the whole sketch and not for individual semantic part in sketches.
In comparison, our dataset annotates for semantic parts in QuickDraw sketches, and we do not limit the annotators with a predefined list of words, so the descriptions tend to be more diverse and creative. There are a total of 1450 different words in our dataset.            
% 200 bird categories with 10,843 images. It includes a training set with 8,326 images in 150 categories and a test set with 2,517 images in the remaining 50 categories.  


\section{Sketch Generation} \label{related.sketch.generate}
%ShadowDraw

\paragraph{Sketch-RNN}
Along with the QuickDraw dataset, \cite{ha2017neural} introduces Sketch-RNN and provides a web demo of collaborative drawing, where users can select a category and draw the first few strokes of a sketch, and the model will complete the rest of the sketch\footnote{You can access the demo from this website \url{https://magenta.tensorflow.org/sketch-rnn-demo}}. Sketch-RNN uses a variational autoencoder (VAE) with bidirectional RNN encoder and RNN decoder. 

\cite{ha2017neural} uses a vector format to represent the sketches: each sketch is a set of strokes, and each stroke is a sequence of points, so the smooth curve is approximated with piecewise linear splines created by connecting adjacent points in the sequence. 
Works in sketch representation learning can be grouped into those that represent the input in this stroke-point vector format and those that treat the entire sketch as a raster image. 
Each point is a 5-component vector, $(\delta x, \delta y, p_1, p_2, p_3)$: the first two components represent changes in the $(x,y)$ coordinate of the current point compared to the last one; the last three make up a one-hot vector representing current state of the sketch. Let $s = \begin{bmatrix}p_1 & p_2 & p_3\end{bmatrix}$, if the current point will be connected with the next point, $s = \begin{bmatrix}1 & 0 & 0\end{bmatrix}$;
if the current point is the last point in the current stroke, so the pen is expected to be lifted next, then $s = \begin{bmatrix}0 & 1 & 0\end{bmatrix}$;
if the current point is the last point in the sketch, then $s = \begin{bmatrix}0 & 0 & 1\end{bmatrix}$.     
The RNN encoder takes this sequence as input and uses the final hidden state $h$ of the RNN to parameterize Gaussian distributions of size $N_z$, from which a latent random variable $z \in \mathbb{R}^{N_z}$ is sampled. 
The latent $z$ will be used to (1) initialize the hidden state $h_0$ of the RNN decoder and (2) be concatenated with the vector at each time step and fed as input into the decoder.  
The RNN decoder predicts parameters for distributions (Gaussian mixture model for sampling $\delta x, \delta y$ and categorical distribution for sampling $p_1,p_2,p_2$) at each time step from the hidden states of the RNN. 
Since the decoder works in an autoregressive manner, it can \textit{encode} the first few strokes provided by the users into a hidden vector, which is conditioned on for later steps to produce distributions of points that make up the rest of the sketch. 

Although our work uses the QuickDraw sketches, we render the vectors into raster images in order to use CLIP and its vision-language joint embeddings, since compared to Sketch-RNN, we need to incorporate text descriptions of the sketch parts.
It is challenging to align semantic information in text with distributions of the $x,y$ coordinates. While language gives high-level guidance on how strokes are organized on canvas to illustrate a particular concept, the coordinate information is too low level if directly used without an intermediate level of abstraction to aggregate these low-level sequence.    

\paragraph{DoodlerGAN} 
DoodlerGAN \citep{doodlerGAN} is a recent work on creative sketching and represents sketches as raster images. It uses Generative Adversarial Network (GAN) to model the sketches\footnote{Try the DoodlerGAN demo here: \url{http://doodlergan.cloudcv.org/}}. 
Compared to Sketch-RNN, which can only complete the missing parts of sketches once users are done with their parts, DoodlerGAN generates sketches using a part-based approach. In this way, users and the system can take multiple turns to create sketches. 

The part-based GAN has two networks: a part selector and a part generator. Given an incomplete sketch with some parts drawn (for example, a bird with its eyes and beak), the part selector determines what kind of part to draw next (for example, the bird wings). Then given the partial sketch and the category of the next part, the part generator produces a raster image of the part and its location on canvas. 
The part generator is a conditional GAN derived from StyleGAN2 \citep{styleGAN2Paper}, and it takes as input an image with number-of-parts$+1$ channels: one channel for each part and an additional channel for the whole partial sketch. The part selector uses the encoder of the part generator with an extra linear layer at the end as the classification head. 

While the part-based generation aspect of DoodlerGAN aligns with our goal to study composition of shapes in sketches, there is no language associated with each part, and the generation process is not guided by language supervision. As indicated by \cite{doodlerGAN}, compared to vector inputs, raster images can take better advantage of spatial structure of sketch parts. We believe that there is a shared abstractness in sketches and language, and we want to explore this aspect by first collecting a (semantic part, language description) dataset and then learning a part-based generative model from language.   

\paragraph{SketchBird}
Along with SketchCUB, \cite{sketchbirds} introduces a GAN-based model that generates sketches of birds from captions. The model architecture is based on AttnGAN \citep{attngan_paper}.
A bidirectional long short-term memory (Bi-LSTM) network is used as the text encoder, which is trained with image-text pairs while minimizing the Deep Attentional Multimodal Similarity Model (DAMSM) loss, proposed in AttnGAN. 
This loss is calculated based on attention-driven text-image matching score, which is the dot-product similarity between words in the caption and image subregions. 

Although SketchBird is one of the few works that look at sketch generation from language, it is limited by the dataset that it is trained and evaluated on: as explained earlier, the SketchCUB dataset does not contain human-drawn sketches, and the captions come from a list of predefined attributes. On the other hand, our work is morivated by how people can express high-level concepts, such as \textit{circular}, through a variety of language, \textit{round}, \textit{moon-shaped}, \textit{ringlike}, \textit{disk-shaped}, and how these type of free-form language can be composed and used to generate abstract sketches, similar to those in the QuickDraw dataset.     

% calculated between a word vector and a weighted sum of vectors of image regions. The weight comes from a matrix of size $T\times 289$ ($T =$ number words in the sentence, and $289 =$ number of image regions), contining using dot-product similarity between word in the sentence and subregion in the image. 

% In terms of exploring the multimodal sketch generation realm (text-to-sketch synthesis), a recent work is SketchBird \citep{sketchbirds}. This work, similar to ours, deal with the unique challenge of generating sketches from textual descriptions. They setup the task to mimic or as a counterpart to the classic text to image generation on the CUB dataset \citep{WahCUB_200_2011}. 
% This work is also representative of a line of work that is based on GAN, unique in the way that it is outputting sketches, closer to the domain that we are interested in.
% The line of text-to-image synthesis work begins with conditional GAN \citep{cGAN_text_image}, which also reports results on the CUB dataset. 
% One thing we are especially interested in is how these models are able to extract the text features, and how they fuse text features with image features. Moreover what loss is used to encourage the alignment between the image and text domain. 

% It seems like from quite a few papers, such as \cite{joyce-chai-zscl}, fuse the visual and textual space by combining the visual features using weights calculated by dot-similarity between the two modality, or vice versa to achieve cross attention. \cite{joyce-chai-zscl} uses a LSTM+GloVE setup for the unimodal text embeddings. 

% What are some other ways that we can extract visually informed text embeddings. 
% StyleGAN-NADA: CLIP Guided Domain Adaptation of Image Generators
% Of course, there are other techniques to generate images from texts, namely, leverage large pretrained model such as GPT-3. GPT-3 and DALL-E are particular nowadays for researchers to replicate on their own and try to query the immense feature space for creative art pieces. However, the abstract art style work is not our focus, and while creativity is an interesting future direction, we emphasize the collaborative aspect more than creativity. 

