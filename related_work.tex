
We survey two areas of related work: text-to-image synthesis and sketch representation learning. There is a long line of works on generating realistic images by learning their underlying distributions, especially on employing generative adversarial networks (GAN). 
Related to our work is a set of works on generating images from text descriptions or use texts to manipulate image styles, but they often work with a constraint set of language, with the exception of industrial-scale generative models with billions of parameters trained from text-image dataset on the hundred million scale.
Most of these work focus on datasets of photo-realistic images, and our work instead centers around sketches, which are abstract since certain features of the objects in the sketches are illustrated figuratively and contain sparse information, as most part of the image is empty. 
Therefore, we also look into the field of sketch representation learning: what methods have been employed and what datasets are constructed.   

% \section{Image Synthesis from Text Prompts}


% \section{Sketch Representation Learning}

\section{Sketch Dataset} \label{related.sketch.datasets}
Since our work seeks to collect a dataset of (sketch parts, text description), we survey exisiting sketch datasets and find that they either lack language descriptions, for the entire sketch or for parts in a sketch, or they do not contain semantic object annotations. 

\paragraph{TU-Berlin}
\citet{eitz2012hdhso} is one of the first works to investigate the characteristics of free-hand sketches and attempt to extract local features based on orietation, which are later used in the task of sketch recognition. It also provides the TU-Berlin sketch dataset that contains $20,000$ sketches spanning 250 object categories with 80 samples in each. The TU-Berlin dataset is then extended for sketch-based 3D object retrieval to contain 1814 new sketches for 130 common household object categories. This additional set also contains hierarchical category labels; for example, the \textit{animal} category contains \textit{arthropod, biped, human, flying creature, quadruped, underwater creature} categories if going one level down the hierarchy. The TU-Berlin contains some of the highest quality sketches for a wide range of categories, but they lack both text descriptions and semantic part annotations, and our dataset contains both types of annotations, although only for simpler sketches in the face and angel category.    

\paragraph{Datasets with Text Descriptions}
The QMUL dataset contains sketches of shoes and chairs: 419 shoe sketches of various types of shoes, such as high heels, boots, and ballerina flats, and 297 chair sketches of different kinds \citep{qmulDataset}. Although QMUL annotates for attributes of the sketches, they come from a fixed set of descriptions and are obtained from product tags, so these descriptions do not reflect how humans would creatively describe the drawings on their mind, which is a feature of our collected dataset.    
The Sketchy dataset contains $75,000$ sketches of 125 categories, and each sketch is paired with an image; Sketchy is additionally annotated with sketch validity metrics and short descriptions from annotators \citep{sketchyDataset}. However, these descriptions are more like comments given by the participants, and the collecting process is not carefully designed, unlike our data collection process, where obtaining the text description is the main focus, ensuring the quality of the language. Moreover, the short texts in Sketchy do not describe individual semantic object in the sketches.     

\paragraph{Quick,Draw! Dataset}
Quick,Draw! collects the largest sketch datasets containing 50 million sketches distributed across 345 object categories, each containing around 100,000 sketch \citep{ha2017neural}. Annotators for the TU-Berlin dataset have 30 minutes to draw one sketch, and annotators for the QMUL dataset have reference photos for the sketches they would draw later; on the other hand, Quick,Draw! participants had 20 seconds to create the sketches for a randomly assigned category without time to look for reference, so the Quick,Draw! sketches are the simplest; however, since sketches that failed to be recognized by a classification model are filtered, they are in general of good quality and are representative of the general population drawing skills. Moreover, many works on sketch representation learning use the Quick,Draw! dataset, so utilize these sketches allow us to potentially adapt some of the pre-trained generative models. 
These features of the Quick,Draw! dataset make it very favorable to be used to learn an collaborative drawing robot that can interact with a wide range of users, including children. However, Quick,Draw! dataset contains neither semantic part annotation nor language descriptions, and our collected dataset contains both in order to build collaborative drawing robots.   

\paragraph{Datasets with Semantic Part Annotations}
The sketches in our dataset come from the \\
Quick,Draw! dataset, but the original Quick,Draw! dataset does not provide labels for semantic objects in the sketches. 
The Sketch Perceptual Grouping (SPG) dataset \citep{spg_paper} and the SketchSeg dataset \citep{sketchsegDataset} contain annotations for semantic segmentation, meaning that each stroke in a sketch is paired with a semantic label; for example, strokes in a sketch for the \textit{face} category are assigned one of the following labels: \textit{eyes, nose, mouth, ear, hair, moustache, outline of face}. 
The SPG dataset annotates for $25,000$ Quick,Draw! sketches, belonging to 25 (out of 345) categories, and it selects 800 out of the original $100,000$ sketches in each category for annotation. 
The SketchSeg dataset builds upon the SPG dataset by using a recurrent neural work to generate additional sketches stemmed from one sketch in SPG; since each stroke in the generated sketch corresponds to a stroke in the original sketch, it automatically has part annotations. However, since the quality of the generated sketch is dependent upon the generative model, we choose the original SPG dataset, which is also publicly available. The SPG dataset does not contain text descriptions like our dataset, so although they can be used to build a collaborative drawing system, such system would not have the capacity to respond to natural language commands from users, and our work is interested in learning collaborative robots that can communicate freely with people.    
%DoodlerGAN

\section{Collaborative Drawing}
%ShadowDraw

\paragraph{Sketch-RNN}
Along with the Quick,Draw! dataset, \cite{ha2017neural} introduces Sketch-RNN and provides a web demo of collaborative drawing, where users can select a category and draw the first few strokes of a sketch, and the model will complete the rest of the sketch\footnote{You can access the demo from this website \url{https://magenta.tensorflow.org/sketch-rnn-demo}}. Sketch-RNN uses a variational autoencoder (VAE) with bi-directional RNN encoder and RNN decoder. 
\cite{ha2017neural} uses a vector format to represent the sketches: each sketch is a set of strokes, and each stroke is a sequence of points, so the smooth curve is approximated with piecewise linear splines created by connecting adjacent points in the sequence. Each point is a 5-component vector, $(\delta x, \delta y, p_1, p_2, p_3)$: the first two components represent changes in the $(x,y)$ coordinate of the current point compared to the last one; the last three make up a one-hot vector representing current state of the sketch. Let $s = \begin{bmatrix}p_1 & p_2 & p_3\end{bmatrix}$,
$ s = \begin{bmatrix}1 & 0 & 0\end{bmatrix}$, if the current point will be connected with the next point;
$s = \begin{bmatrix}0 & 1 & 0\end{bmatrix}$, if the current point is the last point in the current stroke, so the pen is expected to be lifted next;
$s = \begin{bmatrix}0 & 0 & 1\end{bmatrix}$, if the current point is the last point in the sketch.     
The RNN encoder take this sequence as input and uses the final hidden state $h$ of the RNN to parameterize a Gaussian distributions of size $N_z$, from which a latent random variable $z \in \mathbb{R}^{N_z}$ is sampled. The latent $z$ will be used to (1) initialize the hidden state $h_0$ of the RNN decoder and (2) be concatenated with the vector at each time step to be fed as input into the decoder.  
The RNN decoder predicts parameters for distributions (Gaussian mixture model for sampling $\delta x, \delta y$ and categorical distribution for sampling $p_1,p_2,p_2$) at each time step from the hidden states of the RNN. Since the decoder works in an autoregressive manner, meaning that points in a sketch are passed one by one into the RNN, the decoder can \textit{encode} the first few strokes provided by the users into the hidden vector, which is conditioned on for later steps to produce distributions of points that make up the rest of the sketch. 
Subsequent works on sketch representation learning can be grouped into those that represent the input in this stroke-point vector format and those that treat the entire sketch as a raster image. 

Although our work uses the Quick,Draw! sketches, we render the vectors into raster images in order to use CLIP and its vision-language joint embeddings, since compared to Sketch-RNN, we need to incorporate text descriptions of the sketch parts.
It is challenging to align semantic information in text with distributions of the $x,y$ coordinates. While language gives high-level guidance on how strokes are organized on canvas to illustrate a particular concept, the coordinate information is too low level if directly used without an intermediate level of abstraction to aggregate these low-level sequence.    

\paragraph{DoodlerGAN} 
DoodlerGAN \citep{doodlerGAN} is a more recent work on collaborative drawing and represents sketches as raster images; it uses Generative Adversarial Network (GAN) to model the sketches \footnote{Try the DoodlerGAN demo here: \url{http://doodlergan.cloudcv.org/}}. Compared to Sketch-RNN, which can only complete the missing parts of sketches once users are done with their parts, DoodlerGAN generates sketches using a part-based approach so that users and the system can take multiple turns to create sketches more collaboratively. 
Moreover, as indicated by \cite{doodlerGAN}, compared to vector inputs, raster images can take better advantage of information from spatial structure of the parts and ignore effects of shifting coordinates by focusing more on relationships among the sketch parts.   
The part-based GAN has two networks: a part selector and a part generator. Given an incomplete sketch with some parts drawn (for example, a bird with its eyes and beak), the part selector determines what kind of part to draw next (for example, the bird wings). Then given the partial sketch and the category of the next part, the part generator produces a raster image of the part and its location on canvas. 
The part generator is a conditional GAN derived from StyleGAN2, and it takes as input an image with number-of-parts$+1$ channels: one channel for each part and an additional channel for the whole partial sketch. The part selector uses the encoder of the part generator with an extra linear layer at the end as the classification head. 

While the part-based generation aspect of DoodlerGAN aligns with our goal to create collaborative drawing agent, there is no language associated with each part, and the generation process is not guided by language supervision.  

