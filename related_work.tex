
We survey two areas of related work: \todo[size=\small, color=green!20]{change here} text-to-image synthesis and sketch representation learning. There is a long line of works on generating realistic images by learning their underlying distributions, especially on employing generative adversarial networks (GAN). 
Related to our work is a set of works on generating images from text descriptions or use texts to manipulate image styles, but they often work with a constraint set of language, with the exception of industrial-scale generative models with billions of parameters trained from text-image dataset on the hundred million scale. \todo[size=\small, color=green!20]{[X] not very satisfied with this sentence.}%
Most of these work focus on datasets of photo-realistic images, and our work instead centers around sketches, which are abstract since certain features of the objects in the sketches are illustrated figuratively and contain sparse information, as most part of the image is empty. 
Therefore, we also look into the field of sketch representation learning: what methods have been employed and what datasets are constructed.   

% \section{Image Synthesis from Text Prompts}


% \section{Sketch Representation Learning}

\section{Sketch Dataset} \label{related.sketch.datasets}
Since our work seeks to collect a dataset of (sketch parts, text description), we survey exisiting sketch datasets and find that they either lack language descriptions, for the entire sketch or for parts in a sketch, or they do not contain semantic object annotations. 

\paragraph{TU-Berlin}
\citet{eitz2012hdhso} is one of the first works to investigate the characteristics of free-hand sketches and attempt to extract local features based on orietation, which are later used in the task of sketch recognition. It also provides the TU-Berlin sketch dataset that contains $20,000$ sketches spanning 250 object categories with 80 samples in each. The TU-Berlin dataset is then extended for sketch-based 3D object retrieval to contain 1814 new sketches for 130 common household object categories. This additional set also contains hierarchical category labels; for example, the \textit{animal} category contains \textit{arthropod, biped, human, flying creature, quadruped, underwater creature} categories if going one level down the hierarchy. The TU-Berlin contains some of the highest quality sketches for a wide range of categories, but they lack both text descriptions and semantic part annotations, and our dataset contains both types of annotations, although only for simpler sketches in the face and angel category.    

\paragraph{Datasets with Text Descriptions}
The QMUL dataset contains sketches of shoes and chairs: 419 shoe sketches of various types of shoes, such as high heels, boots, and ballerina flats, and 297 chair sketches of different kinds \citep{qmulDataset}. Although QMUL annotates for attributes of the sketches, they come from a fixed set of descriptions and are obtained from product tags, so these descriptions do not reflect how humans would creatively describe the drawings on their mind, which is a feature of our collected dataset.    
The Sketchy dataset contains $75,000$ sketches of 125 categories, and each sketch is paired with an image; Sketchy is additionally annotated with sketch validity metrics and short descriptions from annotators \citep{sketchyDataset}. However, these descriptions are more like comments given by the participants, and the collecting process is not carefully designed, unlike our data collection process, where obtaining the text description is the main focus, ensuring the quality of the language. Moreover, the short texts in Sketchy do not describe individual semantic object in the sketches.     

\paragraph{Quick,Draw! Dataset}
Quick,Draw! collects the largest sketch datasets containing 50 million sketches distributed across 345 object categories, each containing around 100,000 sketch \citep{ha2017neural}. Annotators for the TU-Berlin dataset have 30 minutes to draw one sketch, and annotators for the QMUL dataset have reference photos for the sketches they would draw later; on the other hand, Quick,Draw! participants had 20 seconds to create the sketches for a randomly assigned category without time to look for reference, so the Quick,Draw! sketches are the simplest; however, since sketches that failed to be recognized by a classification model are filtered, they are in general of good quality and are representative of the general population drawing skills. Moreover, many works on sketch representation learning use the Quick,Draw! dataset, so utilize these sketches allow us to potentially adapt some of the pre-trained generative models. 
These features of the Quick,Draw! dataset make it very favorable to be used to learn an collaborative drawing robot that can interact with a wide range of users, including children. However, Quick,Draw! dataset contains neither semantic part annotation nor language descriptions, and our collected dataset contains both in order to build collaborative drawing robots.   

\paragraph{Datasets with Semantic Part Annotations}
The sketches in our dataset come from the \\
Quick,Draw! dataset, but the original Quick,Draw! dataset does not provide labels for semantic objects in the sketches. 
The Sketch Perceptual Grouping (SPG) dataset \citep{spg_paper} and the SketchSeg dataset \citep{sketchsegDataset} contain annotations for semantic segmentation, meaning that each stroke in a sketch is paired with a semantic label; for example, strokes in a sketch for the \textit{face} category are assigned one of the following labels: \textit{eyes, nose, mouth, ear, hair, moustache, outline of face}. 
The SPG dataset annotates for $25,000$ Quick,Draw! sketches, belonging to 25 (out of 345) categories, and it selects 800 out of the original $100,000$ sketches in each category for annotation. 
The SketchSeg dataset builds upon the SPG dataset by using a recurrent neural work to generate additional sketches stemmed from one sketch in SPG; since each stroke in the generated sketch corresponds to a stroke in the original sketch, it automatically has part annotations. However, since the quality of the generated sketch is dependent upon the generative model, we choose the original SPG dataset, which is also publicly available. The SPG dataset does not contain text descriptions like our dataset, so although they can be used to build a collaborative drawing system, such system would not have the capacity to respond to natural language commands from users, and our work is interested in learning collaborative robots that can communicate freely with people.    
%DoodlerGAN

\section{Collaborative Drawing}
%ShadowDraw

\paragraph{Sketch-RNN}
Along with the Quick,Draw! dataset, \cite{ha2017neural} introduces Sketch-RNN and provides a web demo of collaborative drawing, where users can select a category and draw the first few strokes of a sketch, and the model will complete the rest of the sketch\footnote{You can access the demo from this website \url{https://magenta.tensorflow.org/sketch-rnn-demo}}. Sketch-RNN uses a variational autoencoder (VAE) with bi-directional RNN encoder and RNN decoder. 
\cite{ha2017neural} uses a vector format to represent the sketches: each sketch is a set of strokes, and each stroke is a sequence of points, so the smooth curve is approximated with piecewise linear splines created by connecting adjacent points in the sequence. Each point is a 5-component vector, $(\delta x, \delta y, p_1, p_2, p_3)$: the first two components represent changes in the $(x,y)$ coordinate of the current point compared to the last one; the last three make up a one-hot vector representing current state of the sketch. Let $s = \begin{bmatrix}p_1 & p_2 & p_3\end{bmatrix}$,
$ s = \begin{bmatrix}1 & 0 & 0\end{bmatrix}$, if the current point will be connected with the next point;
$s = \begin{bmatrix}0 & 1 & 0\end{bmatrix}$, if the current point is the last point in the current stroke, so the pen is expected to be lifted next;
$s = \begin{bmatrix}0 & 0 & 1\end{bmatrix}$, if the current point is the last point in the sketch.     
The RNN encoder take this sequence as input and uses the final hidden state $h$ of the RNN to parameterize a Gaussian distributions of size $N_z$, from which a latent random variable $z \in \mathbb{R}^{N_z}$ is sampled. The latent $z$ will be used to (1) initialize the hidden state $h_0$ of the RNN decoder and (2) be concatenated with the vector at each time step to be fed as input into the decoder.  
The RNN decoder predicts parameters for distributions (Gaussian mixture model for sampling $\delta x, \delta y$ and categorical distribution for sampling $p_1,p_2,p_2$) at each time step from the hidden states of the RNN. Since the decoder works in an autoregressive manner, meaning that points in a sketch are passed one by one into the RNN, the decoder can \textit{encode} the first few strokes provided by the users into the hidden vector, which is conditioned on for later steps to produce distributions of points that make up the rest of the sketch. 
Subsequent works on sketch representation learning can be grouped into those that represent the input in this stroke-point vector format and those that treat the entire sketch as a raster image. 

Although our work uses the Quick,Draw! sketches, we render the vectors into raster images in order to use CLIP and its vision-language joint embeddings, since compared to Sketch-RNN, we need to incorporate text descriptions of the sketch parts.
It is challenging to align semantic information in text with distributions of the $x,y$ coordinates. While language gives high-level guidance on how strokes are organized on canvas to illustrate a particular concept, the coordinate information is too low level if directly used without an intermediate level of abstraction to aggregate these low-level sequence.    

\paragraph{DoodlerGAN} 
DoodlerGAN \citep{doodlerGAN} is a more recent work on collaborative drawing and represents sketches as raster images; it uses Generative Adversarial Network (GAN) to model the sketches \footnote{Try the DoodlerGAN demo here: \url{http://doodlergan.cloudcv.org/}}. Compared to Sketch-RNN, which can only complete the missing parts of sketches once users are done with their parts, DoodlerGAN generates sketches using a part-based approach so that users and the system can take multiple turns to create sketches more collaboratively. 
Moreover, as indicated by \cite{doodlerGAN}, compared to vector inputs, raster images can take better advantage of information from spatial structure of the parts and ignore effects of shifting coordinates by focusing more on relationships among the sketch parts.   
The part-based GAN has two networks: a part selector and a part generator. Given an incomplete sketch with some parts drawn (for example, a bird with its eyes and beak), the part selector determines what kind of part to draw next (for example, the bird wings). Then given the partial sketch and the category of the next part, the part generator produces a raster image of the part and its location on canvas. 
The part generator is a conditional GAN derived from StyleGAN2, and it takes as input an image with number-of-parts$+1$ channels: one channel for each part and an additional channel for the whole partial sketch. The part selector uses the encoder of the part generator with an extra linear layer at the end as the classification head. 

While the part-based generation aspect of DoodlerGAN aligns with our goal to create collaborative drawing agent, there is no language associated with each part, and the generation process is not guided by language supervision. Therefore, we 

SketchBirds

% In the space of sketch representation learning. After we have settled on human-robot collaborative sketching, we surveyed the field for existing sketch datasets and what they contain, what they lack, and what gap does our work fill. 
Past works have learned sketch representations from a wide variety of tasks: sketch recognition, sketch generation from image, image generation from sketches, sketch retrieval of 3D objects, sketch retrieval of images, semantic segmentation of sketches, etc. 
% Essentially, one can perform any tasks that are done on images and explore the techniques for sketches. 
The survey paper by \citet{sketchsurvey} provides a comprehensive overview and systematic taxonomy of . 
There is a wide range of tasks that can be done on sketches, both unimodal and multimodal, and, for each task, a large reservoir of deep learning methods used to solve the tasks. 
\citet{sketchsurvey} gives a comprehensive review of the task taxonomy, summarized the unique challenges associated with each individual tasks, and evaluated the different deep learning methods on sketch recognition through a library \texttt{TorchSketch} the authors wrote, and it contains implementation for CNN, RNN, GNN, and TCN. The sections that are most relelvant to us are: sketch generation, sketch segmentation. Sketch generation because we are trying to learn a generative model. Sketch segmentation because we are trying to gain insight about how are semantically meaningful units discovered in sketches and what relationships do the parts have with the whole sketch.      
Similar to images, sketches have hierarchical structure, and we 

The hope is that we can leverage previous work on sketch representation learning to gain insights about sketches and how to learn good representations of them. What is unique about sketches compared to regular RGB images from, for example, ImageNet is that (1) sketches are abstract characterisation of the objects, and although humans can recognize and understand a sketch perfectly, they do not necessarily bear big resemblance to their image counterpart; therefore, methods that work well on RGB images, especially generative models like GAN that have successfully generated wide range of images from texts, a realm that we care about, it is not necessarily the case that they can generalize well to our dataset.  




In terms of exploring the multimodal sketch generation realm (text-to-sketch synthesis), a recent work is SketchBird \citep{sketchbirds}. This work, similar to ours, deal with the unique challenge of generating sketches from textual descriptions. They setup the task to mimic or as a counterpart to the classic text to image generation on the CUB dataset \citep{WahCUB_200_2011}. 
This work is also representative of a line of work that is based on GAN, unique in the way that it is outputting sketches, closer to the domain that we are interested in.
The line of text-to-image synthesis work begins with conditional GAN \citep{cGAN_text_image}, which also reports results on the CUB dataset. But what is slightly in lack for the dataset that SketchBirds collected  
But to examine the line of GAN work, we can see that AttnGAN \citep{attngan_paper} (what SketchBirds is based upon or ) 
One thing we are especially interested in is how these models are able to extract the text features, and how they fuse text features with image features. Moreover what loss is used to encourage the alignment between the image and text domain. 
In SketchBirds, a bidirectional long short-term memory (Bi-LSTM) network is used as the text encoder. Inspired by AttnGAN, to extract text vectors that are visually aware, SketchBrids trains the text encoder with image-text pairs while minimizing the Deep Attentional Multimodal Similarity Model (DAMSM) loss, proposed in AttnGAN. This loss is calculated based on attention-driven text-image matching score, where matching is between two vectors, one is the vector representing a word in the sentence, and the other is a weighted sum of vectors of image regions, where the weight comes from a matrix of size $T\times 289$ ($T$ being the number words in the sentence, and $289$ being the number of image regions), calculated using dot-product similarity between word in the sentence and sub-region in the image. 
It seems like from quite a few papers, such as \cite{joyce-chai-zscl}, fuse the visual and textual space by combining the visual features using weights calculated by dot-similarity between the two modality, or vice versa to achieve cross attention. \cite{joyce-chai-zscl} uses a LSTM+GloVE setup for the unimodal text embeddings.       

The SketchCUB dataset collected by SketchBird contains sketches that are more similar to still-life portrait sketches and are very realistic, but sketches in the Quick,Draw! dataset are more similar to icons. This is due to how SketchCUB is transferred from RGB images in the CUB dataset by using open-source holistically-nested network (HED). The SketchCUB dataset contains
200 bird categories with 10,843 images. It includes a training set with 8,326 images in 150 categories and a test set with 2,517 images in the remaining 50 categories.  

What are some other ways that we can extract visually informed text embeddings. 

StyleGAN-NADA: CLIP Guided Domain Adaptation of Image Generators

Of course, there are other techniques to generate images from texts, namely, leverage large pretrained model such as GPT-3. GPT-3 and DALL-E are particular nowadays for researchers to replicate on their own and try to query the immense feature space for creative art pieces. However, the abstract art style work is not our focus, and while creativity is an interesting future direction, we emphasize the collaborative aspect more than creativity. 

%%%%%%%%%%%% outputting basic semantically meaningful components
In the larger realm of RGB images:
Therefore, our dataset will be a good benchmark for how well these models work at capturing the individual semantic components of an object. The reason that we claim this is that some work on GAN's have try to look at how to manipulate certain regions in the images by manipulating the latent space. While this line of work also try to look at how .This area of the work is around facial feature editting. Work such as Semantic Photo Manipulation with a Generative Image Prior \citep{Bau:Ganpaint:2019}, has an interactive interface where the user can use stroke to indicate where in the image they would want a certain object, and the GAN will generate the objects in that location. ``semantic image editing tasks, including synthesizing new objects consistent with background, removing unwanted objects, and changing the appearance of an object''. semantic edit on an object. They would apply a semantic vector space operation in the latent space. How our work is different from this work is that: how well the methods can work on sketches and how well can the edits can done through language. [?] Moreover, it seems like we need to have an image already in order to do the manipulation, but for our ideal tasks, we start from a blank canvas.

