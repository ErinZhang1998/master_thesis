% Background on the different methods on sketch representation learning
% Attribute object word embedding using Glove. \citep{joyce-chai-zscl}
% CLIP embeddings, pre-trained word embedding. Visually informed word embeddings. 
% If given an instruction ``I want to draw circular eyes'', the first question to address is where the eyes should be. Q1: Where should the part starts? This question must have been addressed by the DoodlerGAN paper, since its part generator must have generated the parts conditioned on previous parts. (What is the reason for avoiding GAN training?) The second step is determining how the eyes are drawn on paper. The descriptors will impact the dimension of the objects, such as \textit{small, big, wide, thin}. However, depending on the size of the objects, where you start the object might also be impacted, so where the bounding box starts and how big the bounding box is is dependent upon the language descriptors. Can we \textit{instill} the conditioning idea into existing methods for image generation / sketch generation? 

Our goal is to build a collaborative drawing agent, through communication in language. In the Sketch-RNN work, the model can complete a partial sketch done by the users. In our case, we want to augment this collaborative creative process with language. A user can not only draw part of the sketch but also specify to the robot what kind of sketches they want to create: an angel with cloud-shaped wings or a face with angry-looking eyes. 
Learning sketch representations comes with its own challenges due to its abstractness, but how similar shapes are used in different context and how similar language is used in different context also makes learning sketch representation interesting, especially when most of the state of the art vision-language work focuses on RGB image data, we want to know how well SOTA methods generalize to the sketch domain. To advance towards a collaborative drawing agent, we choose sketch for its abstractness. 
The abstract nature of sketches brings both challenges and simplication of the problem. Challenges are about pretrained large vision-language models like CLIP are trained on RGB image data, simplification is about the fact that things like paintings with brushes bring in more manipulative challenges, so if we do want to study how language interacts with users or users interact with robots, it comes with its own research questions. Therefore, there are a couple of disparate directions that we can take this project. Simplication: it reduces the manipulation challenges with handling different art tools and techniques related to painting.   
\begin{enumerate}
    \item How well does current vision-language joint embeddings capture the abstract sketch representation? We are interested in this question because (1) CLIP is not trained with sketches and is trained mostly with RGB images; (2) humans are able to generalize concepts like small, large  
\end{enumerate}

We utilize CLIP to gain more insights into our dataset, beyond simple counting statistics. Given the nature of our dataset: a large variety of words but most words have very few occurrences, small number of images, text descriptions are contrastively collected by juxtaposing two images with opposite features, similar descriptions used for different purpose in different context. How does CLIP respond to this dataset, how well does CLIP embeddings align with our intuitions about these tasks? Specifically, for transferring the same word to be used in different context, or usage of words that are not the common meaning of words, how well can CLIP handle them, since it is trained on millions of images? Even though CLIP is not on the sketch domain, CLIP was trained on a large number of images on the internet, and there are GAN methods that have taken advantag eof CLIP embeddings to guide image generation: ClipDraw, StylCLIP, CLIP-NADA. 

During data collection, we implicitly juxtapose two sketches, chosen to be as distinct as possible using some heuristic, either from different clusters or whose cosine distance is large, so the process of annotating the two dissimilar sketches is like the annotators are choosing the pair up one annotation with another. 
Implicitly, the annotators is pairing $s_1$ with $t_1$ and $s_2$ with $t_2$, so we would regard the ground truth pairing to be $(s_1,t_1)$ and $(s_2,t_2)$. We want to see how CLIP does on this task, if it is the annotator for the task, would it be able to generate the same pairing. Define cosine similarity to be. 


We will divide this section into explaining the two encoders. When introducing the text encoder, we will briefly introduce the encoder-decoder recurrent neural networks (RNN), which are the state-of-the-art method predating transformers, and their shortcomings in terms of inefficiency handling long sequences; we then introduce the self-attention mechanism and transformer models. In this way, we have better understanding of the advantages of modeling large corpus with transformer models. 
Transformers are first tested in natural language processing tasks and then adapted to vision tasks,



\paragraph{Before Transformer: Recurent Neural Networks}
Traditionally, recurrent neural network's (RNN), such as long short-term memory (LSTM) and gated reccurent unit (GRU) recurrent network, are used in language modeling.
The sequential nature of RNN's precluded them efficient parallelization and resulted in difficulty modeling longer sequences \citep{attentionAllYouNeed}. 
\citet{encoderDecoderRNN} and \citet{encoderDecoderRNN2} introduced the framework of using neural networks, specifically 
RNNs, for language modeling. The framework works as follows:  
if given a sequence of $T$ words $\mathbf{x} =  (x_1,x_2,\dots, x_{T})$, an RNN encoder compute a sequence of hidden states $h_t = f(h_{t-1}, x_t)$, where $h_{t-1}$ is the previous hidden state, and $x_t$ is the current input. It then compresses all the $T$ hidden states into one context vector $c = q(\{h_1,\dots,h_T\})$, where $q$ is some nonlinear function. Therefore, summarizing the input sequence into a representation is carried out in a sequential manner.  
The decoding process is sequential too: $c$ and outputs from previous time steps are used to generate the output of the current step $t$, $y_t = g(c, h_t^{'}, y_1,\dots,y_{t-1})$, where $g$ is some nonlinear transformation, and $h_t^{'}$ is the hidden state in the decoder \citep{encoderDecoderRNN,encoderDecoderRNN2}.
In this way, the encoder-decoder RNN architecture models the output sentence sequence $\mathbf{y}$ as the product of conditionals of each time step: $\mathbf{y} = \prod_{t=1}^T p(y_t | c,y_1,\dots,y_{t-1})$.  
During both encoding and decoding, the RNN architecture constrains the modeling to be sequential \citep{attentionRNN}. Moreover, the sequential nature of RNN limits the ability to model long-range dependencies among parts of the sentences, which is crucial in many cases. 

\paragraph{RNN with Attention}
The attention mechanism, the foudational building block of transformer, has been used with RNN's before. 
Introduced in \citet{attentionRNN}, attention is used to alleviate the problem of compressing all information extracted from the input sequence in one fixed context vector $c$ to compute the output at every time step. In the aforementioned 
$y_t = g(c, h_t^{'}, y_1,\dots,y_{t-1})$, \citet{attentionRNN} altered the fixed $c$ into an hidden state dependent $c_t$, and it is computed as the weighted sum of the encoder hidden states $h_j$: $c_t = \sum_{j=1}^{T} \alpha_{tj} h_j$. The weight $\alpha_{tj}$ represents how well $h_j$, the hidden state corresponding to the $j$-th word in the input, align with the decoder hidden state $h_{t-1}^{'}$ just before the current decoding step $t$. In this way, $c_t$ summarize the input in an output-dependent way, making inputs that associate with $y_t$ more closely contribute more to modeling $ p(y_t | c_t,y_1,\dots,y_{t-1})$. 



It then calculates the cosine similarities between each of the $N \times N$ pairings of the image and text features. The values are used as logit scores for calculating symmetric cross-entropy loss used to train the network. 


CLIP is trained on 400 million (image, text) pairs collected form a variety of publicly available sources on the Internet.

The query list that is used to compile these (image,text) pairs is the union of (1) words that occur $\geq 100$ times in English Wikipedia; (2) names of Wikipedia articles whose search volume is above certain threshold; (2) high pointwise mutual information (PMI) bi-grams, and (4) $117,000$ WordNet synsets, or sets of cognitive synonyms. 


% Let $I_f \in \mathbb{R}^{N \times 512}$ be image embedding for the $N$ images in the batch outputted by the image encoder; similarly, 
% let $T_f \in \mathbb{R}^{N \times 512}$ be text embedding for the $N$ captions in the batch outputted by the text encoder. 
% We then use dot-product to generate.
% \begin{equation}
% \begin{split}
%     \text{image encoder: } E_i \\
%     \text{text encoder: } E_t \\
%     I_1^{'},\dots, I_N^{'} \\
%     T_1^{'},\dots, T_N^{'} \\
%     I_1,\dots, I_N \\
%     T_1,\dots, T_N \\
% \end{split}
% \end{equation}
  
% image logits $X_v$ over the text descriptions and text logits $X_t$ over the images. 






 


    




