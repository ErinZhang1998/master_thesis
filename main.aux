\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand*\new@tpo@label[2]{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{ii}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iii}{chapter*.2}\protected@file@percent }
\citation{allado-mcdowell_okojie_2020}
\citation{gpt3}
\citation{dallePaper,glidePaper,dalle2Paper}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introductionChapter}{{1}{1}{Introduction}{chapter.1}{}}
\zref@newlabel{pdfcomment_soul_markup:1}{\posx{29892319}\posy{17580985}}
\@writefile{lpc}{\contentsline {lpcsec}{\numberline {}\ - }{1}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Triangle is adapted and composed with other shapes to create sketches of different objects. The objects are, from top and in clockwise order: \textit  {ice-cream, cat, house, pine tree, mountain}, and \textit  {boat}.\relax }}{1}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{introduction.composition}{{1.1}{1}{Triangle is adapted and composed with other shapes to create sketches of different objects. The objects are, from top and in clockwise order: \textit {ice-cream, cat, house, pine tree, mountain}, and \textit {boat}.\relax }{figure.caption.5}{}}
\citation{clipDrawPaper,dalle2Paper,styleganNadaPaper,styleCLIPPaper}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces People are likely to provide a variety of descriptions when asked, \textit  {``What features does this cat sketch have?''}.\relax }}{2}{figure.caption.6}\protected@file@percent }
\newlabel{introduction.cat.face}{{1.2}{2}{People are likely to provide a variety of descriptions when asked, \textit {``What features does this cat sketch have?''}.\relax }{figure.caption.6}{}}
\citation{eitz2012hdhso}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{relatedWorkChapter}{{2}{4}{Related Work}{chapter.2}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{green!20}{\leavevmode {\color  {green!20}o}}\ change here}{4}{section*.7}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{16434227}{28016251}
\pgfsyspdfmark {pgfid2}{2753822}{28006147}
\pgfsyspdfmark {pgfid3}{4048158}{27804625}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{green!20}{\leavevmode {\color  {green!20}o}}\ [X] not very satisfied with this sentence.}{4}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid6}{29266147}{22904431}
\pgfsyspdfmark {pgfid7}{2753822}{22894327}
\pgfsyspdfmark {pgfid8}{4048158}{22692805}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sketch Dataset}{4}{section.2.1}\protected@file@percent }
\newlabel{related.sketch.datasets}{{2.1}{4}{Sketch Dataset}{section.2.1}{}}
\citation{qmulDataset}
\citation{sketchyDataset}
\citation{ha2017neural}
\@writefile{toc}{\contentsline {paragraph}{TU-Berlin}{5}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Datasets with Text Descriptions}{5}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quick,Draw! Dataset}{5}{section*.11}\protected@file@percent }
\citation{spg_paper}
\citation{sketchsegDataset}
\citation{ha2017neural}
\citation{ha2017neural}
\@writefile{toc}{\contentsline {paragraph}{Datasets with Semantic Part Annotations}{6}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Collaborative Drawing}{6}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sketch-RNN}{6}{section*.13}\protected@file@percent }
\citation{doodlerGAN}
\citation{doodlerGAN}
\@writefile{toc}{\contentsline {paragraph}{DoodlerGAN}{7}{section*.14}\protected@file@percent }
\citation{sketchsurvey}
\citation{sketchsurvey}
\citation{sketchbirds}
\citation{WahCUB_200_2011}
\citation{cGAN_text_image}
\citation{attngan_paper}
\citation{joyce-chai-zscl}
\citation{joyce-chai-zscl}
\citation{Bau:Ganpaint:2019}
\citation{dallePaper}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Data Collection}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{dataChapter}{{3}{11}{Data Collection}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Prompt-Guided Sketch Text Dataset}{12}{section.3.1}\protected@file@percent }
\newlabel{datav1}{{3.1}{12}{Prompt-Guided Sketch Text Dataset}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Overview}{12}{subsection.3.1.1}\protected@file@percent }
\zref@newlabel{pdfcomment_soul_markup:2}{\posx{16898696}\posy{32172362}}
\@writefile{lpc}{\contentsline {lpcsec}{\numberline {}\ - }{12}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Interface Design}{12}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Main Task Interface}{12}{section*.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A typical annotation process. Top: interface at the start of annotation. Middle: before adding text descriptions for the drawing; red arrow and box show where to click to add text. Bottom: after adding text descriptions for the flower sketch.\relax }}{13}{figure.caption.17}\protected@file@percent }
\newlabel{v1.main_task.1}{{3.1}{13}{A typical annotation process. Top: interface at the start of annotation. Middle: before adding text descriptions for the drawing; red arrow and box show where to click to add text. Bottom: after adding text descriptions for the flower sketch.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Top: the completed annotation for the prompt \textit  {Happy Flower}. Red arrows and boxes point to \textit  {Delete} buttons that can delete the text annotations along with their drawings. Bottom: after deleting the steps \textit  {first round petal} and \textit  {smiley mouth}.\relax }}{14}{figure.caption.18}\protected@file@percent }
\newlabel{v1.main_task.delete}{{3.2}{14}{Top: the completed annotation for the prompt \textit {Happy Flower}. Red arrows and boxes point to \textit {Delete} buttons that can delete the text annotations along with their drawings. Bottom: after deleting the steps \textit {first round petal} and \textit {smiley mouth}.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Instruction and Requirement}{14}{section*.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The instruction section used in the prompt-guided sketch text dataset.\relax }}{15}{figure.caption.20}\protected@file@percent }
\newlabel{v1.instruction}{{3.3}{15}{The instruction section used in the prompt-guided sketch text dataset.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Qualification}{15}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Final version of the requirements. The \leavevmode {\color  {red}\underline  {Bad Example}} links to counter-examples of the requirements, and \leavevmode {\color  {green}\underline  {Good Example}} links to good examples. When turkers click on the links, they are directed to the examples illustrating the corresponding requirement. This design helps them to understand the requirements better and provides high-quality annotations\relax }}{16}{figure.caption.21}\protected@file@percent }
\newlabel{v1.requirement}{{3.4}{16}{Final version of the requirements. The \textcolor {red}{\underline {Bad Example}} links to counter-examples of the requirements, and \textcolor {green}{\underline {Good Example}} links to good examples. When turkers click on the links, they are directed to the examples illustrating the corresponding requirement. This design helps them to understand the requirements better and provides high-quality annotations\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Results}{16}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Contrasting Sketch Text Dataset}{17}{section.3.2}\protected@file@percent }
\newlabel{datav2}{{3.2}{17}{Contrasting Sketch Text Dataset}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Overview}{17}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Interface Design}{18}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Main Task Interface}{18}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Instruction and Requirement}{18}{section*.32}\protected@file@percent }
\citation{ha2017neural}
\citation{spg_paper}
\@writefile{toc}{\contentsline {subsubsection}{Qualification}{19}{section*.36}\protected@file@percent }
\citation{doodlerGAN}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Dataset Summary}{20}{section.3.3}\protected@file@percent }
\newlabel{datasummary}{{3.3}{20}{Dataset Summary}{section.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Dataset statistics by category.\relax }}{20}{table.caption.38}\protected@file@percent }
\newlabel{table:dataset_stats1}{{3.1}{20}{Dataset statistics by category.\relax }{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Dataset statistics by sketch parts. The phrase \textit  {contrasting pair} refers to a pair of sketches with contrasting features that are presented to the annotators for descriptions of their parts.\relax }}{20}{table.caption.39}\protected@file@percent }
\newlabel{table:dataset_stats_byparts}{{3.2}{20}{Dataset statistics by sketch parts. The phrase \textit {contrasting pair} refers to a pair of sketches with contrasting features that are presented to the annotators for descriptions of their parts.\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces This table shows how many times the top-10 most frequently used word in the dataset is used to describe different parts in face and angel sketches. For example, \textit  {small} is used in 192 descriptions for eyes in face sketches, and it is used 150 times to describe angel halos. People use the same words differently depending on the sketching context.\relax }}{21}{table.caption.43}\protected@file@percent }
\newlabel{table:parts_share_description}{{3.3}{21}{This table shows how many times the top-10 most frequently used word in the dataset is used to describe different parts in face and angel sketches. For example, \textit {small} is used in 192 descriptions for eyes in face sketches, and it is used 150 times to describe angel halos. People use the same words differently depending on the sketching context.\relax }{table.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An example used to explain the requirements to turkers.\relax }}{22}{figure.caption.22}\protected@file@percent }
\newlabel{v1.badeg}{{3.5}{22}{An example used to explain the requirements to turkers.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Navigation bar in the qualification test.\relax }}{23}{figure.caption.24}\protected@file@percent }
\newlabel{v1.qualification.nav}{{3.6}{23}{Navigation bar in the qualification test.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces An example of the questions in the qualification test.\relax }}{23}{figure.caption.25}\protected@file@percent }
\newlabel{v1.qualification.q9}{{3.7}{23}{An example of the questions in the qualification test.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Sketches collected for prompts with the descriptor \textit  {sleepy} in the prompt-guided sketch text dataset. People were very creative and came up with various ways to illustrate the concept \textit  {sleepy} in their sketches.\relax }}{24}{figure.caption.26}\protected@file@percent }
\newlabel{v1.sleepy}{{3.8}{24}{Sketches collected for prompts with the descriptor \textit {sleepy} in the prompt-guided sketch text dataset. People were very creative and came up with various ways to illustrate the concept \textit {sleepy} in their sketches.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces A challenge studying creativity in humans: sketching is very subjective, and it is difficult to determine how some sketches illustrate the prompt and judge their annotation quality. This figure shows 4 examples from the prompt-guided sketch text dataset that are hard to interpret.\relax }}{24}{figure.caption.27}\protected@file@percent }
\newlabel{v1.hard_to_understand}{{3.9}{24}{A challenge studying creativity in humans: sketching is very subjective, and it is difficult to determine how some sketches illustrate the prompt and judge their annotation quality. This figure shows 4 examples from the prompt-guided sketch text dataset that are hard to interpret.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces A major issue with the prompt-guided sketch text dataset: many text descriptions do not align with the drawings in each step. In this example, annotations for step 1 and 2 are correct, but in step 3, although the description says the sketch contains \textit  {legs of the cat}, the person actually drew the cat face, ears, whiskers, and legs. Moreover, in step 4, the annotator annotated for parts drawn in the last step. Resolving this problem was very important because we needed semantic part annotations and their corresponding text descriptions to study abstract concept composition.\relax }}{25}{figure.caption.28}\protected@file@percent }
\newlabel{v1.misalign}{{3.10}{25}{A major issue with the prompt-guided sketch text dataset: many text descriptions do not align with the drawings in each step. In this example, annotations for step 1 and 2 are correct, but in step 3, although the description says the sketch contains \textit {legs of the cat}, the person actually drew the cat face, ears, whiskers, and legs. Moreover, in step 4, the annotator annotated for parts drawn in the last step. Resolving this problem was very important because we needed semantic part annotations and their corresponding text descriptions to study abstract concept composition.\relax }{figure.caption.28}{}}
\newlabel{v2.main_task.1.a}{{3.11a}{26}{In the first pilot, we ask annotators to describe differences between the two sketches in full sentence.\relax }{figure.caption.29}{}}
\newlabel{sub@v2.main_task.1.a}{{a}{26}{In the first pilot, we ask annotators to describe differences between the two sketches in full sentence.\relax }{figure.caption.29}{}}
\newlabel{v2.main_task.1.b}{{3.11b}{26}{Compared to the previous pilot (\ref {v2.main_task.1.a}), we still ask explicitly the differences between the sketches but limit annotations to adjective phrases.\relax }{figure.caption.29}{}}
\newlabel{sub@v2.main_task.1.b}{{b}{26}{Compared to the previous pilot (\ref {v2.main_task.1.a}), we still ask explicitly the differences between the sketches but limit annotations to adjective phrases.\relax }{figure.caption.29}{}}
\newlabel{v2.main_task.1.d}{{3.11c}{27}{Similar to the last pilot (\ref {v2.main_task.1.b}), the final version asks for adjective phrases, but we do not state explicitly that annotators should describe the visual differences to allow for more creativity.\relax }{figure.caption.30}{}}
\newlabel{sub@v2.main_task.1.d}{{c}{27}{Similar to the last pilot (\ref {v2.main_task.1.b}), the final version asks for adjective phrases, but we do not state explicitly that annotators should describe the visual differences to allow for more creativity.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Different versions of the main task interface in chronological order. Final version used to collect the entire dataset is \ref  {v2.main_task.1.d}.\relax }}{27}{figure.caption.30}\protected@file@percent }
\newlabel{v2.main_task.1}{{3.11}{27}{Different versions of the main task interface in chronological order. Final version used to collect the entire dataset is \ref {v2.main_task.1.d}.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces A pop-up window containing examples of adjective phrases, used in collecting contrasting sketch text dataset in Section \ref  {datav2}. It can be opened from the side panel by clicking on \leavevmode {\color  {blue}\it  {some examples}}. Note that we used examples that are unrelated to our task on purpose, because annotators tended to repeat words in examples, so we tried to not bias them.\relax }}{28}{figure.caption.33}\protected@file@percent }
\newlabel{v2.adjective.phrases}{{3.12}{28}{A pop-up window containing examples of adjective phrases, used in collecting contrasting sketch text dataset in Section \ref {datav2}. It can be opened from the side panel by clicking on \textcolor {blue}{\it {some examples}}. Note that we used examples that are unrelated to our task on purpose, because annotators tended to repeat words in examples, so we tried to not bias them.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Requirements used in collecting the contrasting sketch text dataset (Section \ref  {datav2}).\relax }}{28}{figure.caption.34}\protected@file@percent }
\newlabel{v2.requirement.1}{{3.13}{28}{Requirements used in collecting the contrasting sketch text dataset (Section \ref {datav2}).\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces 2 examples used in the instructions to collect the contrasting sketch text dataset (Section \ref  {datav2}).\relax }}{29}{figure.caption.35}\protected@file@percent }
\newlabel{v2.examples}{{3.14}{29}{2 examples used in the instructions to collect the contrasting sketch text dataset (Section \ref {datav2}).\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Question 3 from the qualification test used to collect the contrasting sketch text dataset (Section \ref  {datav2}). We show a hint at the beginning of each question telling the annotators which requirement this question is testing. In this way, we encourage them to review the requirements so that they have a good understanding of the task and can provide high-quality annotations in the real HIT. The question interface is the same as main task interface that annotators will see when they annotate. The 1-to-1 mock-up helps them to be familiar with the workflow.\relax }}{30}{figure.caption.37}\protected@file@percent }
\newlabel{v2.qualification.1}{{3.15}{30}{Question 3 from the qualification test used to collect the contrasting sketch text dataset (Section \ref {datav2}). We show a hint at the beginning of each question telling the annotators which requirement this question is testing. In this way, we encourage them to review the requirements so that they have a good understanding of the task and can provide high-quality annotations in the real HIT. The question interface is the same as main task interface that annotators will see when they annotate. The 1-to-1 mock-up helps them to be familiar with the workflow.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Top 100 most frequent words in the dataset corpus.\relax }}{31}{figure.caption.40}\protected@file@percent }
\newlabel{word_freq}{{3.16}{31}{Top 100 most frequent words in the dataset corpus.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces .\relax }}{32}{figure.caption.41}\protected@file@percent }
\newlabel{datasummary.circles.varied_language}{{3.17}{32}{.\relax }{figure.caption.41}{}}
\newlabel{datasummary.face.varied_language}{{3.18a}{33}{.\relax }{figure.caption.42}{}}
\newlabel{sub@datasummary.face.varied_language}{{a}{33}{.\relax }{figure.caption.42}{}}
\newlabel{datasummary.angel.varied_language}{{3.18b}{33}{.\relax }{figure.caption.42}{}}
\newlabel{sub@datasummary.angel.varied_language}{{b}{33}{.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces .\relax }}{33}{figure.caption.42}\protected@file@percent }
\newlabel{datasummary.face_angel.varied_language}{{3.18}{33}{.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Left: face sketches whose hair descriptions contain the word \textit  {spiky}. In faces, the word refers to short and coarse texture of hair. Right: angel sketches whose wing description contains the word \textit  {spiky}, which is used to represent the pointy edges on wings.\relax }}{34}{figure.caption.44}\protected@file@percent }
\newlabel{datasummary.spiky}{{3.19}{34}{Left: face sketches whose hair descriptions contain the word \textit {spiky}. In faces, the word refers to short and coarse texture of hair. Right: angel sketches whose wing description contains the word \textit {spiky}, which is used to represent the pointy edges on wings.\relax }{figure.caption.44}{}}
\citation{clipDrawPaper,styleCLIPPaper,styleganNadaPaper,dalle2Paper}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Modeling}{35}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{modelingChapter}{{4}{35}{Modeling}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Task Definition}{35}{section.4.1}\protected@file@percent }
\newlabel{modeling.task.def}{{4.1}{35}{Task Definition}{section.4.1}{}}
\newlabel{modeling.task.sketches.1}{{4.1a}{35}{$t_1$: \textit {wide triangular body}\relax }{figure.caption.45}{}}
\newlabel{sub@modeling.task.sketches.1}{{a}{35}{$t_1$: \textit {wide triangular body}\relax }{figure.caption.45}{}}
\newlabel{modeling.task.sketches.2}{{4.1b}{35}{$t_2$: \textit {small rectangular body}\relax }{figure.caption.45}{}}
\newlabel{sub@modeling.task.sketches.2}{{b}{35}{$t_2$: \textit {small rectangular body}\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Two angel sketches, $s_1$ on the left and $s_2$ on the right, and their part annotations, $t_1$ and $t_2$. The task is for CLIP to determine which sketch matches a given $t_j$.\relax }}{35}{figure.caption.45}\protected@file@percent }
\newlabel{modeling.task.sketches}{{4.1}{35}{Two angel sketches, $s_1$ on the left and $s_2$ on the right, and their part annotations, $t_1$ and $t_2$. The task is for CLIP to determine which sketch matches a given $t_j$.\relax }{figure.caption.45}{}}
\citation{CLIPpaper}
\citation{CLIPpaper}
\citation{CLIPpaper}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Method}{36}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Contrastive Objective}{36}{subsection.4.2.1}\protected@file@percent }
\newlabel{clip.objective}{{4.2.1}{36}{Contrastive Objective}{subsection.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces CLIP uses contrastive instead of generative objective for pre-training to learn joint vision-language embeddings. This image is taken from Figure 1 in the original CLIP paper \citep  {CLIPpaper}. Each grid contains the dot-product of normalized image,text features.\relax }}{36}{figure.caption.46}\protected@file@percent }
\newlabel{modeling.clip.pretrainingobj}{{4.2}{36}{CLIP uses contrastive instead of generative objective for pre-training to learn joint vision-language embeddings. This image is taken from Figure 1 in the original CLIP paper \citep {CLIPpaper}. Each grid contains the dot-product of normalized image,text features.\relax }{figure.caption.46}{}}
\citation{CLIPpaper}
\newlabel{cliploss.image}{{4.1}{37}{Contrastive Objective}{equation.4.2.1}{}}
\newlabel{cliploss.text}{{4.2}{37}{Contrastive Objective}{equation.4.2.2}{}}
\newlabel{cliploss.all}{{4.3}{37}{Contrastive Objective}{equation.4.2.3}{}}
\citation{attentionAllYouNeed}
\citation{attentionAllYouNeed}
\citation{CLIPpaper}
\citation{CLIPpaper}
\citation{attentionAllYouNeed}
\citation{attentionAllYouNeed}
\citation{attentionAllYouNeed}
\citation{Radford2019LanguageMA}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Text Encoder}{38}{subsection.4.2.2}\protected@file@percent }
\newlabel{modeling.transformer.origEncoder}{{4.3a}{38}{Encoder architecture used in original Transformer paper; the figure is taken from Figure 1 in \cite {attentionAllYouNeed}.\relax }{figure.caption.48}{}}
\newlabel{sub@modeling.transformer.origEncoder}{{a}{38}{Encoder architecture used in original Transformer paper; the figure is taken from Figure 1 in \cite {attentionAllYouNeed}.\relax }{figure.caption.48}{}}
\newlabel{modeling.attention.clipEncoder}{{4.3b}{38}{Encoder architecture of CLIP \citep {CLIPpaper}.\relax }{figure.caption.48}{}}
\newlabel{sub@modeling.attention.clipEncoder}{{b}{38}{Encoder architecture of CLIP \citep {CLIPpaper}.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Text encoder architecture: the text encoder is a transformer with stacked layers of self-attention and feed-forward network sublayers. The main implementation difference between the original Transformer (on the left) and the transformer implemented by CLIP is the placement of layer normalization.\relax }}{38}{figure.caption.48}\protected@file@percent }
\newlabel{modeling.transformer}{{4.3}{38}{Text encoder architecture: the text encoder is a transformer with stacked layers of self-attention and feed-forward network sublayers. The main implementation difference between the original Transformer (on the left) and the transformer implemented by CLIP is the placement of layer normalization.\relax }{figure.caption.48}{}}
\citation{attentionAllYouNeed}
\citation{attentionAllYouNeed}
\citation{attentionAllYouNeed}
\citation{attentionAllYouNeed}
\citation{CLIPpaper}
\citation{attentionAllYouNeed}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An illustration of the scaled dot-product attention on the left; on the right, an illustration of the multi-head self-attention mechanism (MSA) used in every layer of the transformer. Both figures are from the original Transformer paper \citep  {attentionAllYouNeed}.\relax }}{39}{figure.caption.49}\protected@file@percent }
\newlabel{modeling.attention}{{4.4}{39}{An illustration of the scaled dot-product attention on the left; on the right, an illustration of the multi-head self-attention mechanism (MSA) used in every layer of the transformer. Both figures are from the original Transformer paper \citep {attentionAllYouNeed}.\relax }{figure.caption.49}{}}
\citation{attentionAllYouNeed}
\citation{ViT}
\citation{ViT}
\citation{ViT}
\citation{attentionAllYouNeed}
\citation{ViT}
\newlabel{mha}{{4.4}{40}{Multi-Head Self-Attention}{equation.4.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Vision Transformer}{40}{subsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Vision Transformer (ViT) architecture \citep  {ViT}.\relax }}{40}{figure.caption.51}\protected@file@percent }
\newlabel{modeling.visionTransformer}{{4.5}{40}{Vision Transformer (ViT) architecture \citep {ViT}.\relax }{figure.caption.51}{}}
\citation{ViT}
\citation{ha2017neural}
\citation{spg_paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Fine-Tuning CLIP}{41}{subsection.4.2.4}\protected@file@percent }
\newlabel{text.preprocess}{{4.2.4}{41}{Text Pre-Processing}{section*.53}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results \& Analysis}{43}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{analysisChapter}{{5}{43}{Results \& Analysis}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Classification Experiment}{43}{section.5.1}\protected@file@percent }
\newlabel{results.acc}{{5.1}{43}{Classification Experiment}{section.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Accuracy (\%) of CLIP on choosing the correct sketch for a given text description from a pair of sketches. During annotation, the annotator is given the pair of sketches and provided descriptions of certain parts in the sketches for both sketches.\relax }}{44}{table.caption.54}\protected@file@percent }
\newlabel{results.clip.acc}{{5.1}{44}{Accuracy (\%) of CLIP on choosing the correct sketch for a given text description from a pair of sketches. During annotation, the annotator is given the pair of sketches and provided descriptions of certain parts in the sketches for both sketches.\relax }{table.caption.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Word Similarity Experiment}{45}{section.5.2}\protected@file@percent }
\newlabel{results.cosinesim}{{5.2}{45}{Word Similarity Experiment}{section.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example pair of sketches with different eyes that was presented to annotators. We do not pre-define a list of attributes and ask the annotators to determine whether the sketches are different in these attributes; instead, we highlight the parts in colors and implicitly prompt them to pick up on the differences. As expected, most annotations naturally include the size and shape differences. From the descriptions, we can extract pairs of \textit  {antonyms} used to indicate opposite visual concepts. In this case, the annotation was \textit  {large circular eyes} for the sketch on the left and \textit  {tiny solid eyes} for the one on the right.\relax }}{45}{figure.caption.55}\protected@file@percent }
\newlabel{results.contrasting.sketches}{{5.1}{45}{An example pair of sketches with different eyes that was presented to annotators. We do not pre-define a list of attributes and ask the annotators to determine whether the sketches are different in these attributes; instead, we highlight the parts in colors and implicitly prompt them to pick up on the differences. As expected, most annotations naturally include the size and shape differences. From the descriptions, we can extract pairs of \textit {antonyms} used to indicate opposite visual concepts. In this case, the annotation was \textit {large circular eyes} for the sketch on the left and \textit {tiny solid eyes} for the one on the right.\relax }{figure.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces 20 pairs of descriptors most frequently used by annotators to differentiate parts in face and angel sketches. There are a total of 9823 pairs, and 7194, or $73.2\%$, pairs are used only once in the dataset.\relax }}{46}{table.caption.56}\protected@file@percent }
\newlabel{results.clip.top20.sim}{{5.2}{46}{20 pairs of descriptors most frequently used by annotators to differentiate parts in face and angel sketches. There are a total of 9823 pairs, and 7194, or $73.2\%$, pairs are used only once in the dataset.\relax }{table.caption.56}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Average cosine similarity.\relax }}{46}{table.caption.57}\protected@file@percent }
\newlabel{results.wordsim}{{5.3}{46}{Average cosine similarity.\relax }{table.caption.57}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Work}{48}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{futureChapter}{{6}{48}{Future Work}{chapter.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces .\relax }}{48}{figure.caption.58}\protected@file@percent }
\newlabel{results.hookNose}{{6.1}{48}{.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Designing the Requirement Section for the Prompt-Guided Sketch Text Dataset}{50}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendixDataV1Req}{{A}{50}{Designing the Requirement Section for the Prompt-Guided Sketch Text Dataset}{appendix.A}{}}
\newlabel{v1.requirement_1.1}{{A.1a}{51}{An example included in the first version of the requirements, explained in more details in item \ref {v1_requirement1_3}.\relax }{figure.caption.59}{}}
\newlabel{sub@v1.requirement_1.1}{{a}{51}{An example included in the first version of the requirements, explained in more details in item \ref {v1_requirement1_3}.\relax }{figure.caption.59}{}}
\newlabel{v1_requirement1_3}{{3}{51}{Designing the Requirement Section for the Prompt-Guided Sketch Text Dataset}{Item.7}{}}
\newlabel{v1.requirement_1.2}{{A.1b}{52}{Unaligned drawing and text description.\relax }{figure.caption.60}{}}
\newlabel{sub@v1.requirement_1.2}{{b}{52}{Unaligned drawing and text description.\relax }{figure.caption.60}{}}
\newlabel{v1.requirement_1.3}{{A.1c}{52}{An example of misalignment: text description \textit {overflow} into multiple steps.\relax }{figure.caption.60}{}}
\newlabel{sub@v1.requirement_1.3}{{c}{52}{An example of misalignment: text description \textit {overflow} into multiple steps.\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Screenshots of counterexamples used in first version of the requirements in Version 1.\relax }}{52}{figure.caption.60}\protected@file@percent }
\newlabel{v1.requirement_1}{{A.1}{52}{Screenshots of counterexamples used in first version of the requirements in Version 1.\relax }{figure.caption.60}{}}
\bibstyle{apacite}
\bibdata{thesiscite}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Screenshots of counterexamples used in third version of the requirements.\relax }}{53}{figure.caption.61}\protected@file@percent }
\newlabel{v1.requirement_3}{{A.2}{53}{Screenshots of counterexamples used in third version of the requirements.\relax }{figure.caption.61}{}}
\bibcite{allado-mcdowell_okojie_2020}{{1}{{\APACyear {2020}}}{{Allado-McDowell\ \&{} Okojie}}{{Allado-McDowell\ \&{} Okojie}}}
\bibcite{Bau:Ganpaint:2019}{{2}{{\APACyear {2019}}}{{Bau\ \BOthers {.}}}{{Bau\ \BOthers {.}}}}
\bibcite{gpt3}{{3}{{\APACyear {2020}}}{{Brown\ \BOthers {.}}}{{Brown\ \BOthers {.}}}}
\bibcite{ViT}{{4}{{\APACyear {2020}}}{{Dosovitskiy\ \BOthers {.}}}{{Dosovitskiy\ \BOthers {.}}}}
\bibcite{eitz2012hdhso}{{5}{{\APACyear {2012}}}{{Eitz\ \BOthers {.}}}{{Eitz, Hays,{}\ \&{} Alexa}}}
\bibcite{clipDrawPaper}{{6}{{\APACyear {2021}}}{{Frans\ \BOthers {.}}}{{Frans, Soros,{}\ \&{} Witkowski}}}
\bibcite{styleganNadaPaper}{{7}{{\APACyear {2021}}}{{Gal\ \BOthers {.}}}{{Gal, Patashnik, Maron, Chechik,{}\ \&{} Cohen-Or}}}
\bibcite{doodlerGAN}{{8}{{\APACyear {2020}}}{{Ge\ \BOthers {.}}}{{Ge, Goswami, Zitnick,{}\ \&{} Parikh}}}
\bibcite{ha2017neural}{{9}{{\APACyear {2017}}}{{Ha\ \&{} Eck}}{{Ha\ \&{} Eck}}}
\bibcite{spg_paper}{{10}{{\APACyear {2018}}}{{Li\ \BOthers {.}}}{{Li\ \BOthers {.}}}}
\bibcite{glidePaper}{{11}{{\APACyear {2021}}}{{Nichol\ \BOthers {.}}}{{Nichol\ \BOthers {.}}}}
\bibcite{styleCLIPPaper}{{12}{{\APACyear {2021}}}{{Patashnik\ \BOthers {.}}}{{Patashnik, Wu, Shechtman, Cohen-Or,{}\ \&{} Lischinski}}}
\bibcite{sketchsegDataset}{{13}{{\APACyear {2019}}}{{Qi\ \&{} Tan}}{{Qi\ \&{} Tan}}}
\bibcite{CLIPpaper}{{14}{{\APACyear {2021}}}{{Radford\ \BOthers {.}}}{{Radford\ \BOthers {.}}}}
\bibcite{Radford2019LanguageMA}{{15}{{\APACyear {2019}}}{{Radford\ \BOthers {.}}}{{Radford\ \BOthers {.}}}}
\bibcite{dalle2Paper}{{16}{{\APACyear {2022}}}{{Ramesh\ \BOthers {.}}}{{Ramesh, Dhariwal, Nichol, Chu,{}\ \&{} Chen}}}
\bibcite{dallePaper}{{17}{{\APACyear {2021}}}{{Ramesh\ \BOthers {.}}}{{Ramesh\ \BOthers {.}}}}
\bibcite{cGAN_text_image}{{18}{{\APACyear {2016}}}{{Reed\ \BOthers {.}}}{{Reed\ \BOthers {.}}}}
\bibcite{sketchyDataset}{{19}{{\APACyear {2016}}}{{Sangkloy\ \BOthers {.}}}{{Sangkloy, Burnell, Ham,{}\ \&{} Hays}}}
\bibcite{attentionAllYouNeed}{{20}{{\APACyear {2017}}}{{Vaswani\ \BOthers {.}}}{{Vaswani\ \BOthers {.}}}}
\bibcite{WahCUB_200_2011}{{21}{{\APACyear {2011}}}{{Wah\ \BOthers {.}}}{{Wah, Branson, Welinder, Perona,{}\ \&{} Belongie}}}
\bibcite{joyce-chai-zscl}{{22}{{\APACyear {2021}}}{{G.~Xu\ \BOthers {.}}}{{G.~Xu, Kordjamshidi,{}\ \&{} Chai}}}
\bibcite{sketchsurvey}{{23}{{\APACyear {2020}}}{{P.~Xu\ \BOthers {.}}}{{P.~Xu\ \BOthers {.}}}}
\bibcite{attngan_paper}{{24}{{\APACyear {2017}}}{{T.~Xu\ \BOthers {.}}}{{T.~Xu\ \BOthers {.}}}}
\bibcite{qmulDataset}{{25}{{\APACyear {2016}}}{{Yu\ \BOthers {.}}}{{Yu\ \BOthers {.}}}}
\bibcite{sketchbirds}{{26}{{\APACyear {2021}}}{{Yuan\ \BOthers {.}}}{{Yuan\ \BOthers {.}}}}
\ulp@afterend
\gdef \@abspage@last{61}
